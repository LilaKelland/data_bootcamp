{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e036b-116b-4eca-b876-542dedf78b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/value-stream-design/online-machine-learning-515556ff72c5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582788b9-670f-41c6-bf87-865e25a991c8",
   "metadata": {},
   "source": [
    "\n",
    "* online learning (also known as incremental or out-of-core learning).\n",
    "* libraries - Vowpal Wabbit\n",
    "* sklearn creme\n",
    "\n",
    "Performance gets better over time\n",
    "\n",
    "ML are static (train then parameters static - dont touch again).  Scalable horiontally.\n",
    "\n",
    "If new data, traing again - if in environments where behaviours change quickly. Online shopping is one such environment: a product that is popular today may be all but forgotten tomorrow.\n",
    "\n",
    "In order to react to new data and make an AI that learns over time, ML practitioners typically do one of two things:\n",
    "* They manually train on newer data, and deploy the resulting model once they are happy with its performance\n",
    "* They schedule training on new data to take place, say, once a week and automatically deploy the resulting model\n",
    "\n",
    "BUT want to Not only predict in real time, but learn in real time, too.\n",
    "\n",
    "#### Example\n",
    "I usually start with the SGDRegressor class. Here’s how to train a simple model on dummy data (taken straight from the scikit documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774e1e1b-8b44-4f21-9f4e-c72513fa3241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "n_samples, n_features = 10, 5\n",
    "y = np.random.randn(n_samples)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "clf = linear_model.SGDRegressor()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec72a2f-d6cb-44f7-a983-c5e80c4754e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29340604])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  fit does training, \n",
    "# predict:\n",
    "clf.predict(np.random.randn(1, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab860bb-652b-46cb-b373-a223a7f84db3",
   "metadata": {},
   "source": [
    "In addition to the fit()method, the SGDRegressor also provides a partial_fit() method, so that you can incrementally train on small batches of data. In fact, all learning algorithms that are compatible with standard optimisation algorithms like (stochastic) gradient decent, adam, RMSprop, and so on have this capability.\n",
    "\n",
    "Out of curiosity, let’s see how long it takes to train on a single example using partial_fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5260408e-d7a2-481a-ade2-57be81de75f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010979175567626953\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "n_samples, n_features = 1, 500\n",
    "y = np.random.randn(n_samples)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "clf = linear_model.SGDRegressor()\n",
    "import time\n",
    "start_time = time.time()\n",
    "clf.partial_fit(X, y)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95609274-7bc5-4a70-b3ed-4d189df8f3f3",
   "metadata": {},
   "source": [
    "Therein lies the potential of online learning: the second we see a new example, let’s learn from it as fast as we can. The faster, the better. In fact, because speed trumps everything else in online learning, we typically use simple learning algorithms over complex ones like neural networks. We strive for millisecond-level learning; everything else comes second.\n",
    "\n",
    "example where good:  govenment issue state of emergency - click on websites\n",
    "but day after - still recommending domestic news\n",
    "\n",
    "That’s the power of online learning: done properly, it can react in minutes or even seconds. With it, there is no such thing as “yesterday’s news”.\n",
    "\n",
    "but if open up to internet - so many ways it can go wrong - over, underfitting, skewing , DD0S\n",
    "\n",
    "more prone to catastrophi interference than other methods\n",
    "\n",
    " It’s not horizontally scalable. Instead, you are forced to have a single model instance that eats new data as fast as it can, spitting out sets of learned parameters behind an API. And the second that one set in one process gets replaced by a new one, all other processes must follow suit immediately. It’s an engineering challenge, because the most important part (the model) is only vertically scalable. It may not even be feasible to distribute between threads.\n",
    " \n",
    "Needs access to data - in-memry store like Redis.  Beig data processing framewors not much wowrkd (Spark uselesss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd052e-3318-40c2-b261-002589bc4c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_bootcamp",
   "language": "python",
   "name": "new_bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
