{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f778e99e-2ea5-4b44-9dbc-f599c42b7bb7",
   "metadata": {},
   "source": [
    "## hyperperamters\n",
    "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f05a26-c38e-479b-8ee8-db23e29ff53d",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression does not really have any critical hyperparameters to tune.\n",
    "\n",
    "Sometimes, you can see useful differences in performance or convergence with different solvers (solver).\n",
    "\n",
    "* solver in [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’]\n",
    "\n",
    "Regularization (penalty) can sometimes be helpful.\n",
    "\n",
    "* penalty in [‘none’, ‘l1’, ‘l2’, ‘elasticnet’]\n",
    "Note: not all solvers support all regularization terms.\n",
    "\n",
    "The C parameter controls the penality strength, which can also be effective.\n",
    "\n",
    "* C in [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e027089-2e82-4458-bda8-7ea71b8dbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.960000 using {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.954333 (0.014761) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.952000 (0.015578) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.952333 (0.016265) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.954667 (0.015434) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.953000 (0.015308) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.952667 (0.015691) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.955333 (0.014996) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.955000 (0.016073) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.954333 (0.014302) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.956333 (0.016224) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.956333 (0.016224) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.955667 (0.013828) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.960000 (0.015492) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.960000 (0.015492) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.958333 (0.012134) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# example of grid searching key hyperparametres for logistic regression\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b2adc-5d4b-4475-b32b-316410eefbc8",
   "metadata": {},
   "source": [
    "### Ridge Classifier\n",
    "Ridge regression is a penalized linear regression model for predicting a numerical value.\n",
    "\n",
    "Nevertheless, it can be very effective when applied to classification.\n",
    "\n",
    "Perhaps the most important parameter to tune is the regularization strength (alpha). A good starting point might be values in the range [0.1 to 1.0]\n",
    "\n",
    "* alpha in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041253d8-b753-4003-8491-2dcf9458d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.978667 using {'alpha': 0.1}\n",
      "0.978667 (0.016275) with: {'alpha': 0.1}\n",
      "0.978667 (0.016275) with: {'alpha': 0.2}\n",
      "0.978667 (0.016275) with: {'alpha': 0.3}\n",
      "0.978667 (0.016275) with: {'alpha': 0.4}\n",
      "0.978667 (0.016275) with: {'alpha': 0.5}\n",
      "0.978667 (0.016275) with: {'alpha': 0.6}\n",
      "0.978667 (0.016275) with: {'alpha': 0.7}\n",
      "0.978667 (0.016275) with: {'alpha': 0.8}\n",
      "0.978667 (0.016275) with: {'alpha': 0.9}\n",
      "0.978667 (0.016275) with: {'alpha': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# example of grid searching key hyperparametres for ridge classifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "# define dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define models and parameters\n",
    "model = RidgeClassifier()\n",
    "alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "# define grid search\n",
    "grid = dict(alpha=alpha)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49388cd-274f-45c6-ad66-108a3516b079",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)\n",
    "The most important hyperparameter for KNN is the number of neighbors (n_neighbors).\n",
    "\n",
    "Test values between at least 1 and 21, perhaps just the odd numbers.\n",
    "\n",
    "* n_neighbors in [1 to 21]\n",
    "It may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood.\n",
    "\n",
    "* metric in [‘euclidean’, ‘manhattan’, ‘minkowski’]\n",
    "For a fuller list see:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n",
    "\n",
    "It may also be interesting to test the contribution of members of the neighborhood via different weightings (weights).\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3688bd36-6f30-4baa-9b19-812dd3d1dc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.966667 using {'metric': 'euclidean', 'n_neighbors': 17, 'weights': 'uniform'}\n",
      "0.868000 (0.029143) with: {'metric': 'euclidean', 'n_neighbors': 1, 'weights': 'uniform'}\n",
      "0.868000 (0.029143) with: {'metric': 'euclidean', 'n_neighbors': 1, 'weights': 'distance'}\n",
      "0.914667 (0.025131) with: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "0.914667 (0.025131) with: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "0.940333 (0.016630) with: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n",
      "0.940333 (0.016630) with: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "0.951667 (0.017528) with: {'metric': 'euclidean', 'n_neighbors': 7, 'weights': 'uniform'}\n",
      "0.951667 (0.017528) with: {'metric': 'euclidean', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "0.950333 (0.017026) with: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n",
      "0.950333 (0.017026) with: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'distance'}\n",
      "0.958333 (0.016750) with: {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "0.958333 (0.016750) with: {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'distance'}\n",
      "0.960333 (0.019746) with: {'metric': 'euclidean', 'n_neighbors': 13, 'weights': 'uniform'}\n",
      "0.960333 (0.019746) with: {'metric': 'euclidean', 'n_neighbors': 13, 'weights': 'distance'}\n",
      "0.965000 (0.019279) with: {'metric': 'euclidean', 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "0.965000 (0.019279) with: {'metric': 'euclidean', 'n_neighbors': 15, 'weights': 'distance'}\n",
      "0.966667 (0.018679) with: {'metric': 'euclidean', 'n_neighbors': 17, 'weights': 'uniform'}\n",
      "0.966667 (0.018679) with: {'metric': 'euclidean', 'n_neighbors': 17, 'weights': 'distance'}\n",
      "0.963667 (0.019405) with: {'metric': 'euclidean', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "0.963667 (0.019405) with: {'metric': 'euclidean', 'n_neighbors': 19, 'weights': 'distance'}\n",
      "0.871000 (0.028089) with: {'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'uniform'}\n",
      "0.871000 (0.028089) with: {'metric': 'manhattan', 'n_neighbors': 1, 'weights': 'distance'}\n",
      "0.913333 (0.026119) with: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "0.913333 (0.026119) with: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "0.934333 (0.026543) with: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'uniform'}\n",
      "0.934333 (0.026543) with: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "0.941000 (0.023993) with: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'uniform'}\n",
      "0.941000 (0.023993) with: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "0.945333 (0.021561) with: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'uniform'}\n",
      "0.945333 (0.021561) with: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\n",
      "0.954333 (0.017065) with: {'metric': 'manhattan', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "0.954333 (0.017065) with: {'metric': 'manhattan', 'n_neighbors': 11, 'weights': 'distance'}\n",
      "0.957333 (0.019310) with: {'metric': 'manhattan', 'n_neighbors': 13, 'weights': 'uniform'}\n",
      "0.957333 (0.019310) with: {'metric': 'manhattan', 'n_neighbors': 13, 'weights': 'distance'}\n",
      "0.957667 (0.019093) with: {'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "0.957667 (0.019093) with: {'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'distance'}\n",
      "0.958667 (0.016479) with: {'metric': 'manhattan', 'n_neighbors': 17, 'weights': 'uniform'}\n",
      "0.958667 (0.016479) with: {'metric': 'manhattan', 'n_neighbors': 17, 'weights': 'distance'}\n",
      "0.961000 (0.018138) with: {'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "0.961000 (0.018138) with: {'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'distance'}\n",
      "0.868000 (0.029143) with: {'metric': 'minkowski', 'n_neighbors': 1, 'weights': 'uniform'}\n",
      "0.868000 (0.029143) with: {'metric': 'minkowski', 'n_neighbors': 1, 'weights': 'distance'}\n",
      "0.914667 (0.025131) with: {'metric': 'minkowski', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "0.914667 (0.025131) with: {'metric': 'minkowski', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "0.940333 (0.016630) with: {'metric': 'minkowski', 'n_neighbors': 5, 'weights': 'uniform'}\n",
      "0.940333 (0.016630) with: {'metric': 'minkowski', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "0.951667 (0.017528) with: {'metric': 'minkowski', 'n_neighbors': 7, 'weights': 'uniform'}\n",
      "0.951667 (0.017528) with: {'metric': 'minkowski', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "0.950333 (0.017026) with: {'metric': 'minkowski', 'n_neighbors': 9, 'weights': 'uniform'}\n",
      "0.950333 (0.017026) with: {'metric': 'minkowski', 'n_neighbors': 9, 'weights': 'distance'}\n",
      "0.958333 (0.016750) with: {'metric': 'minkowski', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "0.958333 (0.016750) with: {'metric': 'minkowski', 'n_neighbors': 11, 'weights': 'distance'}\n",
      "0.960333 (0.019746) with: {'metric': 'minkowski', 'n_neighbors': 13, 'weights': 'uniform'}\n",
      "0.960333 (0.019746) with: {'metric': 'minkowski', 'n_neighbors': 13, 'weights': 'distance'}\n",
      "0.965000 (0.019279) with: {'metric': 'minkowski', 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "0.965000 (0.019279) with: {'metric': 'minkowski', 'n_neighbors': 15, 'weights': 'distance'}\n",
      "0.966667 (0.018679) with: {'metric': 'minkowski', 'n_neighbors': 17, 'weights': 'uniform'}\n",
      "0.966667 (0.018679) with: {'metric': 'minkowski', 'n_neighbors': 17, 'weights': 'distance'}\n",
      "0.963667 (0.019405) with: {'metric': 'minkowski', 'n_neighbors': 19, 'weights': 'uniform'}\n",
      "0.963667 (0.019405) with: {'metric': 'minkowski', 'n_neighbors': 19, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example of grid searching key hyperparametres for KNeighborsClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# define dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define models and parameters\n",
    "model = KNeighborsClassifier()\n",
    "n_neighbors = range(1, 21, 2)\n",
    "weights = ['uniform', 'distance']\n",
    "metric = ['euclidean', 'manhattan', 'minkowski']\n",
    "# define grid search\n",
    "grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e46d00-3112-4dee-97bf-85b565f72ea6",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "The SVM algorithm, like gradient boosting, is very popular, very effective, and provides a large number of hyperparameters to tune.\n",
    "\n",
    "Perhaps the first important parameter is the choice of kernel that will control the manner in which the input variables will be projected. There are many to choose from, but linear, polynomial, and RBF are the most common, perhaps just linear and RBF in practice.\n",
    "\n",
    "* kernels in [‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’]\n",
    "If the polynomial kernel works out, then it is a good idea to dive into the degree hyperparameter.\n",
    "\n",
    "Another critical parameter is the penalty (C) that can take on a range of values and has a dramatic effect on the shape of the resulting regions for each class. A log scale might be a good starting point.\n",
    "\n",
    "* C in [100, 10, 1.0, 0.1, 0.001]\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8556542-5269-4d41-936a-bc20d46ed4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.977667 using {'C': 0.1, 'gamma': 'scale', 'kernel': 'sigmoid'}\n",
      "0.972000 (0.015362) with: {'C': 50, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.975667 (0.015206) with: {'C': 50, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.957000 (0.017916) with: {'C': 50, 'gamma': 'scale', 'kernel': 'sigmoid'}\n",
      "0.972000 (0.015362) with: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.975667 (0.015206) with: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.967333 (0.018607) with: {'C': 10, 'gamma': 'scale', 'kernel': 'sigmoid'}\n",
      "0.971000 (0.016603) with: {'C': 1.0, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.977000 (0.015948) with: {'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.974333 (0.013085) with: {'C': 1.0, 'gamma': 'scale', 'kernel': 'sigmoid'}\n",
      "0.731333 (0.065966) with: {'C': 0.1, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.973667 (0.016017) with: {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.977667 (0.012297) with: {'C': 0.1, 'gamma': 'scale', 'kernel': 'sigmoid'}\n",
      "0.731333 (0.065966) with: {'C': 0.01, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "0.974000 (0.014048) with: {'C': 0.01, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.974000 (0.015406) with: {'C': 0.01, 'gamma': 'scale', 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "# example of grid searching key hyperparametres for SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "# define dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define model and parameters\n",
    "model = SVC()\n",
    "kernel = ['poly', 'rbf', 'sigmoid']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "gamma = ['scale']\n",
    "# define grid search\n",
    "grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b6774-1fb4-40c3-8559-0aae0fd085c2",
   "metadata": {},
   "source": [
    "### Bagged Decision Trees (Bagging)\n",
    "The most important parameter for bagged decision trees is the number of trees (n_estimators).\n",
    "\n",
    "Ideally, this should be increased until no further improvement is seen in the model.\n",
    "\n",
    "Good values might be a log scale from 10 to 1,000.\n",
    "\n",
    "* n_estimators in [10, 100, 1000]\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09af594f-6942-46b6-ba1a-06913ee88e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.930667 using {'n_estimators': 1000}\n",
      "0.850667 (0.038981) with: {'n_estimators': 10}\n",
      "0.915667 (0.030842) with: {'n_estimators': 100}\n",
      "0.930667 (0.025682) with: {'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# example of grid searching key hyperparameters for BaggingClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# define dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define models and parameters\n",
    "model = BaggingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0af33-1cab-446d-8c8f-1ae96e286c51",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "The most important parameter is the number of random features to sample at each split point (max_features).\n",
    "\n",
    "You could try a range of integer values, such as 1 to 20, or 1 to half the number of input features.\n",
    "\n",
    "* max_features [1 to 20]\n",
    "Alternately, you could try a suite of different default value calculators.\n",
    "\n",
    "* max_features in [‘sqrt’, ‘log2’]\n",
    "Another important parameter for random forest is the number of trees (n_estimators).\n",
    "\n",
    "Ideally, this should be increased until no further improvement is seen in the model.\n",
    "\n",
    "Good values might be a log scale from 10 to 1,000.\n",
    "\n",
    "* n_estimators in [10, 100, 1000]\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604272df-1d8e-4b55-8d9c-05a2ffac2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of grid searching key hyperparameters for RandomForestClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# define dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define models and parameters\n",
    "model = RandomForestClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "max_features = ['sqrt', 'log2']\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators,max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da260c-55d4-46f0-aec9-6e9551e5cf80",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "Also called Gradient Boosting Machine (GBM) or named for the specific implementation, such as XGBoost.\n",
    "\n",
    "The gradient boosting algorithm has many parameters to tune.\n",
    "\n",
    "There are some parameter pairings that are important to consider. The first is the learning rate, also called shrinkage or eta (learning_rate) and the number of trees in the model (n_estimators). Both could be considered on a log scale, although in different directions.\n",
    "\n",
    "* learning_rate in [0.001, 0.01, 0.1]\n",
    "* n_estimators [10, 100, 1000]\n",
    "\n",
    "Another pairing is the number of rows or subset of the data to consider for each tree (subsample) and the depth of each tree (max_depth). These could be grid searched at a 0.1 and 1 interval respectively, although common values can be tested directly.\n",
    "\n",
    "* subsample in [0.5, 0.7, 1.0]\n",
    "* max_depth in [3, 7, 9]\n",
    "\n",
    "https://machinelearningmastery.com/configure-gradient-boosting-algorithm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21492ceb-b27e-4054-9a48-706930ad7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of grid searching key hyperparameters for GradientBoostingClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# define dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define models and parameters\n",
    "model = GradientBoostingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
