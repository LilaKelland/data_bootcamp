{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9cf8df5-6e9c-4161-b63b-5614472fbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as spstats\n",
    "%matplotlib inline\n",
    "\n",
    "# from sklearn.datasets import pokemon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf4f20d-70c4-447b-af71-933dbb0ce35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poke_df = pd.read_csv('datasets/Pokemon.csv', encoding='utf-8') \n",
    "# poke_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4c47e-2f8e-48e9-8a3b-7dd0ddbea845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valuse\n",
    "poke_df[['HP', 'Attack', 'Defense']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d7e403-84f3-41d7-b97f-75e224b939fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "poke_df[['HP', 'Attack', 'Defense']].describe()\n",
    "# With this you can get a good idea about statistical measures in these features like count, \n",
    "# average, standard deviation and quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b241fc-13b5-4d88-8815-7a52b2a97e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts\n",
    "popsong_df = pd.read_csv('datasets/song_views.csv', \n",
    "                          encoding='utf-8')\n",
    "popsong_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b136e-a14e-4ad9-9759-29a695e8f7fc",
   "metadata": {},
   "source": [
    "Binarization\n",
    "\n",
    "Often raw frequencies or counts may not be relevant for building a model based on the problem which is being solved. For instance if I’m building a recommendation system for song recommendations, I would just want to know if a person is interested or has listened to a particular song. This doesn’t require the number of times a song has been listened to since I am more concerned about the various songs he\\she has listened to. In this case, a binary feature is preferred as opposed to a count based feature. We can binarize our listen_count field as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2529b-6d47-40c2-9c4d-939153b763a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "watched = np.array(popsong_df['listen_count']) \n",
    "watched[watched >= 1] = 1\n",
    "popsong_df['watched'] = watched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98680753-6dd0-462c-b553-48ee8a4bbded",
   "metadata": {},
   "source": [
    "You can also use scikit-learn's Binarizer class here from its preprocessing module to perform the same task instead of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd4e0b-be38-46c2-a929-999638322a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "bn = Binarizer(threshold=0.9)\n",
    "pd_watched = bn.transform([popsong_df['listen_count']])[0]\n",
    "popsong_df['pd_watched'] = pd_watched\n",
    "popsong_df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c994a0-b32b-4e15-aff4-9d56df5c6b02",
   "metadata": {},
   "source": [
    "Rounding\n",
    "\n",
    "Often when dealing with continuous numeric attributes like proportions or percentages, we may not need the raw values having a high amount of precision. Hence it often makes sense to round off these high precision percentages into numeric integers. These integers can then be directly used as raw values or even as categorical (discrete-class based) features. Let’s try applying this concept in a dummy dataset depicting store items and their popularity percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1189f-d270-4efc-a515-7bf30f47d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_popularity = pd.read_csv('datasets/item_popularity.csv',  \n",
    "                               encoding='utf-8')\n",
    "items_popularity['popularity_scale_10'] = np.array(\n",
    "                   np.round((items_popularity['pop_percent'] * 10)),  \n",
    "                   dtype='int')\n",
    "items_popularity['popularity_scale_100'] = np.array(\n",
    "                  np.round((items_popularity['pop_percent'] * 100)),    \n",
    "                  dtype='int')\n",
    "items_popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6cd65-8e28-48a0-80e1-f9c27898671c",
   "metadata": {},
   "source": [
    "Interactions\n",
    "\n",
    "Supervised machine learning models usually try to model the output responses (discrete classes or continuous values) as a function of the input feature variables. For example, a simple linear regression equation can be depicted as\n",
    "\n",
    "where the input features are depicted by variables\n",
    "\n",
    "having weights or coefficients denoted by\n",
    "\n",
    "respectively and the goal is to predict the response y.\n",
    "In this case, this simple linear model depicts the relationship between the output and inputs, purely based on the individual, separate input features.\n",
    "However, often in several real-world scenarios, it makes sense to also try and capture the interactions between these feature variables as a part of the input feature set. A simple depiction of the extension of the above linear regression formulation with interaction features would be\n",
    "\n",
    "where the features represented by\n",
    "\n",
    "denote the interaction features. Let’s try engineering some interaction features on our Pokémon dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13931aab-7193-4a4e-8d8f-9011209902c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_def = poke_df[['Attack', 'Defense']]\n",
    "atk_def.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ae560-536b-4b6f-a6c6-c45f7e7f499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=2, interaction_only=False,  \n",
    "                        include_bias=False)\n",
    "res = pf.fit_transform(atk_def)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c5365f-9c42-4411-84af-5c60dfa9023d",
   "metadata": {},
   "source": [
    "The above feature matrix depicts a total of five features including the new interaction features. We can see the degree of each feature in the above matrix as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236cd34-3f20-4607-b2f2-c3ccdee7e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pf.powers_, columns=['Attack_degree',  \n",
    "                                  'Defense_degree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a80af9-8d72-4b4e-9ec9-b4d90328f2e3",
   "metadata": {},
   "source": [
    "we can assign a name to each feature now as follows. This is just for ease of understanding and you should name your features with better, easy to access and simple names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec99e1-d65c-4e57-b1ae-75110fa654fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "intr_features = pd.DataFrame(res, columns=['Attack', 'Defense',  \n",
    "                                           'Attack^2', \n",
    "                                           'Attack x Defense',  \n",
    "                                           'Defense^2'])\n",
    "intr_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d57b2-212f-4809-8b19-31d41657e1d7",
   "metadata": {},
   "source": [
    "Binning\n",
    "\n",
    "The problem of working with raw, continuous numeric features is that often the distribution of values in these features will be skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ceaf0e-b259-4af0-be1b-e568b679c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcc_survey_df = pd.read_csv('datasets/fcc_2016_coder_survey_subset.csv', \n",
    "encoding='utf-8')\n",
    "fcc_survey_df[['ID.x', 'EmploymentField', 'Age', 'Income']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30530a3-598c-4096-a963-a20d004fb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed-Width Binning\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Age'].hist(color='#A9C5D3', edgecolor='black',  \n",
    "                          grid=False)\n",
    "ax.set_title('Developer Age Histogram', fontsize=12)\n",
    "ax.set_xlabel('Age', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f139e-62f6-4a05-be4b-394132f1cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now assign ranges into bins 0..6 etc\n",
    "# like with rounding, take floor value after /10\n",
    "\n",
    "fcc_survey_df['Age_bin_round'] = np.array(np.floor(\n",
    "                              np.array(fcc_survey_df['Age']) / 10.))\n",
    "fcc_survey_df[['ID.x', 'Age', 'Age_bin_round']].iloc[1071:1076]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11886c-1e88-4ec3-b702-31508d3a8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want to decide and fix the bin widths based on our own rules\\logic? Binning based on custom \n",
    "# ranges will help us achieve this. \n",
    "# Let’s define some custom age ranges for binning developer ages using the following scheme\n",
    "# 0-15 - 1, 16-30 - 2...\n",
    "\n",
    "bin_ranges = [0, 15, 30, 45, 60, 75, 100]\n",
    "bin_names = [1, 2, 3, 4, 5, 6]\n",
    "fcc_survey_df['Age_bin_custom_range'] = pd.cut(\n",
    "                                           np.array(\n",
    "                                              fcc_survey_df['Age']), \n",
    "                                              bins=bin_ranges)\n",
    "fcc_survey_df['Age_bin_custom_label'] = pd.cut(\n",
    "                                           np.array(\n",
    "                                              fcc_survey_df['Age']), \n",
    "                                              bins=bin_ranges,            \n",
    "                                              labels=bin_names)\n",
    "# view the binned features \n",
    "fcc_survey_df[['ID.x', 'Age', 'Age_bin_round', \n",
    "               'Age_bin_custom_range',   \n",
    "               'Age_bin_custom_label']].iloc[10a71:1076]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f2c66-2b03-4253-a93f-fdbd44e098d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Binning\n",
    "# we use the data distribution itself to decide our bin ranges. (Quartile based binning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f1fa5-d498-424d-9d44-12989e8bb76c",
   "metadata": {},
   "source": [
    "q-Quantiles help in partitioning a numeric attribute into q equal partitions. Popular examples of quantiles include the 2-Quantile known as the median which divides the data distribution into two equal bins, 4-Quantiles known as the quartiles which divide the data into 4 equal bins and 10-Quantiles also known as the deciles which create 10 equal width bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e12014-050c-47f5-bb8d-9a9e658e3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at income field \n",
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3', \n",
    "                             edgecolor='black', grid=False)\n",
    "ax.set_title('Developer Income Histogram', fontsize=12)\n",
    "ax.set_xlabel('Developer Income', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a22fc0-1152-4b73-9ff5-5940c0749f0f",
   "metadata": {},
   "source": [
    "The above distribution depicts a right skew in the income with lesser developers earning more money and vice versa. Let’s take a 4-Quantile or a quartile based adaptive binning scheme. We can obtain the quartiles easily as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fb1ce-ffd1-4e92-b4da-befa72f558f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_list = [0, .25, .5, .75, 1.]\n",
    "quantiles = fcc_survey_df['Income'].quantile(quantile_list)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b291e7-eb5b-4278-9ba8-4ef61d3c5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3', \n",
    "                             edgecolor='black', grid=False)\n",
    "for quantile in quantiles:\n",
    "    qvl = plt.axvline(quantile, color='r')\n",
    "ax.legend([qvl], ['Quantiles'], fontsize=10)\n",
    "ax.set_title('Developer Income Histogram with Quantiles', \n",
    "             fontsize=12)\n",
    "ax.set_xlabel('Developer Income', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# plots with quartiles on the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4fd8a-2fa7-4fb2-91ec-1b095af3faa8",
   "metadata": {},
   "source": [
    "The red lines in the distribution above depict the quartile values and our potential bins. Let’s now leverage this knowledge to build our quartile based binning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fc970-a734-4dde-86ba-76ac035b63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']\n",
    "fcc_survey_df['Income_quantile_range'] = pd.qcut(\n",
    "                                            fcc_survey_df['Income'], \n",
    "                                            q=quantile_list)\n",
    "fcc_survey_df['Income_quantile_label'] = pd.qcut(\n",
    "                                            fcc_survey_df['Income'], \n",
    "                                            q=quantile_list,       \n",
    "                                            labels=quantile_labels)\n",
    "\n",
    "fcc_survey_df[['ID.x', 'Age', 'Income', 'Income_quantile_range', \n",
    "               'Income_quantile_label']].iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee2fc32-a55f-4b45-b7d5-5948f0e780ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... There's more (log and box-cox transform)\n",
    "# https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b1e77-7d87-435b-a544-082e5603abf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc09e0-9f13-47d1-9415-49d6386b13fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
