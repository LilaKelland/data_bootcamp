{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Topic Modeling Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:12.932082Z",
     "start_time": "2020-04-29T12:18:12.200358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lilakelland/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lilakelland/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import TfidfVectorizer and CountVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# import NMF and LatentDirichletAllocation from sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import re\n",
    "\n",
    "#NLTK\n",
    "import nltk #spacy \n",
    "nltk.download('punkt') #download if you have not done so\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords') #download if you have not done so\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ***********NEED TO CLEAN FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instatiate tokenizer\n",
    "tokenizer = word_tokenize\n",
    "\n",
    "#get stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['.',',',\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:14.463840Z",
     "start_time": "2020-04-29T12:18:13.020189Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to remove stop words, \\n, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a variable called `'no_features'` and set its value to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:15.590700Z",
     "start_time": "2020-04-29T12:18:15.585820Z"
    }
   },
   "outputs": [],
   "source": [
    "no_features = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a variable `'no_topics'` and set its value to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:16.743304Z",
     "start_time": "2020-04-29T12:18:16.737763Z"
    }
   },
   "outputs": [],
   "source": [
    "no_topics =100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate a TfidfVectorizer with the following parameters:\n",
    "\n",
    "\n",
    "    * max_df = 0.95\n",
    "    * min_df = 2\n",
    "    * max_features = no_features\n",
    "    * stop_words = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:17.892838Z",
     "start_time": "2020-04-29T12:18:17.888668Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.95, \n",
    "                         min_df=2,\n",
    "                         max_features=no_features,\n",
    "                         stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use fit_transform method of TfidfVectorizer to transform the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:21.486038Z",
     "start_time": "2020-04-29T12:18:19.135830Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf_ft= tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get the features names from TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:22.661253Z",
     "start_time": "2020-04-29T12:18:22.656169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '10', '12', '14', '15', '16', '20', '25', 'a86', 'available',\n",
       "       'ax', 'b8f', 'believe', 'best', 'better', 'bit', 'case', 'com',\n",
       "       'come', 'course', 'data', 'day', 'did', 'didn', 'different',\n",
       "       'does', 'doesn', 'don', 'drive', 'edu', 'fact', 'far', 'file',\n",
       "       'g9v', 'god', 'going', 'good', 'got', 'government', 'help',\n",
       "       'information', 'jesus', 'just', 'key', 'know', 'law', 'let',\n",
       "       'like', 'line', 'list', 'little', 'll', 'long', 'look', 'lot',\n",
       "       'mail', 'make', 'max', 'mr', 'need', 'new', 'number', 'people',\n",
       "       'point', 'power', 'probably', 'problem', 'program', 'question',\n",
       "       'read', 'really', 'right', 'run', 'said', 'say', 'second', 'set',\n",
       "       'software', 'space', 'state', 'sure', 'tell', 'thanks', 'thing',\n",
       "       'things', 'think', 'time', 'true', 'try', 'use', 'used', 'using',\n",
       "       've', 'want', 'way', 'windows', 'work', 'world', 'year', 'years'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate NMF and fit transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:24.532755Z",
     "start_time": "2020-04-29T12:18:23.661009Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=no_topics, random_state=1, alpha_W=.1, l1_ratio=.5, init='nndsvd').fit_transform(tfidf_ft)\n",
    "# alpha_H?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA w/ Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate a CountVectorizer with following parameters:\n",
    "\n",
    "\n",
    "    * max_df = 0.95\n",
    "    * min_df = 2\n",
    "    * max_features = no_features\n",
    "    * stop_words = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:25.547906Z",
     "start_time": "2020-04-29T12:18:25.540452Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use fit_transform method of CountVectorizer to transform documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:29.307223Z",
     "start_time": "2020-04-29T12:18:26.637153Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_ft = cv.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get the features names from TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:31.516381Z",
     "start_time": "2020-04-29T12:18:31.498740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '10', '12', '14', '15', '16', '20', '25', 'a86', 'available',\n",
       "       'ax', 'b8f', 'believe', 'best', 'better', 'bit', 'case', 'com',\n",
       "       'come', 'course', 'data', 'day', 'did', 'didn', 'different',\n",
       "       'does', 'doesn', 'don', 'drive', 'edu', 'fact', 'far', 'file',\n",
       "       'g9v', 'god', 'going', 'good', 'got', 'government', 'help',\n",
       "       'information', 'jesus', 'just', 'key', 'know', 'law', 'let',\n",
       "       'like', 'line', 'list', 'little', 'll', 'long', 'look', 'lot',\n",
       "       'mail', 'make', 'max', 'mr', 'need', 'new', 'number', 'people',\n",
       "       'point', 'power', 'probably', 'problem', 'program', 'question',\n",
       "       'read', 'really', 'right', 'run', 'said', 'say', 'second', 'set',\n",
       "       'software', 'space', 'state', 'sure', 'tell', 'thanks', 'thing',\n",
       "       'things', 'think', 'time', 'true', 'try', 'use', 'used', 'using',\n",
       "       've', 'want', 'way', 'windows', 'work', 'world', 'year', 'years'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate LatentDirichletAllocation and fit transformed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:03.315322Z",
     "start_time": "2020-04-29T12:18:32.768365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001     , 0.001     , 0.001     , ..., 0.001     , 0.001     ,\n",
       "        0.001     ],\n",
       "       [0.00111111, 0.00111111, 0.00111111, ..., 0.00111111, 0.00111111,\n",
       "        0.00111111],\n",
       "       [0.00111111, 0.00111111, 0.00111111, ..., 0.00111111, 0.00111111,\n",
       "        0.00111111],\n",
       "       ...,\n",
       "       [0.00333333, 0.00333333, 0.00333333, ..., 0.00333333, 0.00333333,\n",
       "        0.00333333],\n",
       "       [0.002     , 0.002     , 0.002     , ..., 0.002     , 0.002     ,\n",
       "        0.002     ],\n",
       "       [0.04051348, 0.0004    , 0.0004    , ..., 0.0004    , 0.0004    ,\n",
       "        0.0004    ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, \n",
    "                                max_iter=5, \n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit_transform(cv_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a function `display_topics` that is able to display the top words in a topic for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:04.476192Z",
     "start_time": "2020-04-29T12:19:04.469045Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic_point_state_right_need_long',\n",
       " 'topic_day_tell_like_going_just',\n",
       " 'topic_sure_point_want_good_help',\n",
       " 'topic_think_need_best_00_a86',\n",
       " 'topic_ax_max_b8f_g9v_a86']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lda\n",
    "n_words = 10\n",
    "topic_list = []\n",
    "feature_names = cv.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    top_n = [feature_names[i]\n",
    "             for i in topic.argsort()\n",
    "             [-n_words:]][::-1]\n",
    "    top_features = ' '.join(top_n)\n",
    "    topic_list.append(f\"topic_{'_'.join(top_n[:5])}\") \n",
    "\n",
    "#     print(f\"Topic {topic_idx}: {top_features}\")\n",
    "    \n",
    "topic_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* display top 10 words from each topic from NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:05.672461Z",
     "start_time": "2020-04-29T12:19:05.656545Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q4/f3xj1m3j2h3fl1lc7921wysh0000gn/T/ipykernel_25507/1858353570.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q4/f3xj1m3j2h3fl1lc7921wysh0000gn/T/ipykernel_25507/4240447002.py\u001b[0m in \u001b[0;36mdisplay_topics\u001b[0;34m(model, feature_names, num_top_words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     for topic_idx, topic in enumerate(model.components_):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Topic %d:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         print(\" \".join([feature_names[i]\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "display_topics(nmf, cv.get_feature_names_out(), 10)\n",
    "# WHY IS THIS NOT WORKING???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* display top 10 words from each topic from LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:06.842806Z",
     "start_time": "2020-04-29T12:19:06.831721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "point state right need long second fact does people things\n",
      "Topic 1:\n",
      "day tell like going just read problem need think know\n",
      "Topic 2:\n",
      "sure point want good help did government say use question\n",
      "Topic 3:\n",
      "think need best 00 a86 tell want using new people\n",
      "Topic 4:\n",
      "ax max b8f g9v a86 14 mr ll 25 probably\n",
      "Topic 5:\n",
      "god true say jesus believe things people does did know\n",
      "Topic 6:\n",
      "said second years tell work new right ll true like\n",
      "Topic 7:\n",
      "probably look like tell need used point want long run\n",
      "Topic 8:\n",
      "available software run like new need people used probably work\n",
      "Topic 9:\n",
      "mail list like new time tell does different use 15\n",
      "Topic 10:\n",
      "think 16 program point com space case let probably use\n",
      "Topic 11:\n",
      "help set use like does tell things work used new\n",
      "Topic 12:\n",
      "com know g9v way mr let look don point try\n",
      "Topic 13:\n",
      "don like know just think say ll tell need try\n",
      "Topic 14:\n",
      "time years long like far just work make better people\n",
      "Topic 15:\n",
      "good like tell just day think probably lot believe right\n",
      "Topic 16:\n",
      "just right like use don true make doesn did got\n",
      "Topic 17:\n",
      "government come set help state bit jesus sure point little\n",
      "Topic 18:\n",
      "edu mail com file available software line data use list\n",
      "Topic 19:\n",
      "make case like does look good don point thanks state\n",
      "Topic 20:\n",
      "long file run does look 20 bit program course 14\n",
      "Topic 21:\n",
      "00 new 20 15 10 list 25 thing help day\n",
      "Topic 22:\n",
      "law fact right government state people new just used come\n",
      "Topic 23:\n",
      "case probably space run doesn point sure little year want\n",
      "Topic 24:\n",
      "second used fact b8f point really 12 things data 25\n",
      "Topic 25:\n",
      "program run use need time look like new world used\n",
      "Topic 26:\n",
      "got data line jesus thanks right need problem long far\n",
      "Topic 27:\n",
      "try space set max mr said file right 00 far\n",
      "Topic 28:\n",
      "think mr just like going ll does time ve use\n",
      "Topic 29:\n",
      "read try work need long help right true time does\n",
      "Topic 30:\n",
      "ve better run like need got long does tell just\n",
      "Topic 31:\n",
      "data second use run years need 16 new long time\n",
      "Topic 32:\n",
      "used believe power ll going did b8f need program good\n",
      "Topic 33:\n",
      "used work information want world lot 12 doesn year windows\n",
      "Topic 34:\n",
      "doesn government really people need com bit believe set tell\n",
      "Topic 35:\n",
      "available case sure things second want com time thing 25\n",
      "Topic 36:\n",
      "best need does like use right far power time new\n",
      "Topic 37:\n",
      "know does like need help don thanks doesn just mail\n",
      "Topic 38:\n",
      "drive use need new time does got going better run\n",
      "Topic 39:\n",
      "doesn help didn say don make said better need file\n",
      "Topic 40:\n",
      "believe far things new question doesn true got god second\n",
      "Topic 41:\n",
      "want better try work things does time just 14 program\n",
      "Topic 42:\n",
      "information work does new don available know government thing years\n",
      "Topic 43:\n",
      "course tell need just new ve don year good think\n",
      "Topic 44:\n",
      "new ll like need time just tell going got used\n",
      "Topic 45:\n",
      "bit like think don need just really way know different\n",
      "Topic 46:\n",
      "way make work true long tell better think good got\n",
      "Topic 47:\n",
      "list thing space bit max set tell years didn going\n",
      "Topic 48:\n",
      "problem try run tell help work long new edu using\n",
      "Topic 49:\n",
      "going sure make work right things like long just know\n",
      "Topic 50:\n",
      "00 using say ll ax let file number law far\n",
      "Topic 51:\n",
      "day mail max think say problem ll using thanks set\n",
      "Topic 52:\n",
      "10 12 14 20 25 like second 15 new list\n",
      "Topic 53:\n",
      "got didn like just tell long things going used know\n",
      "Topic 54:\n",
      "doesn try day true want way thanks good long ve\n",
      "Topic 55:\n",
      "going com best power time make 12 space law things\n",
      "Topic 56:\n",
      "question true like think look work know want need case\n",
      "Topic 57:\n",
      "information know got b8f drive 00 things use years little\n",
      "Topic 58:\n",
      "windows power like run need tell just help 14 using\n",
      "Topic 59:\n",
      "using use work run like need does just time don\n",
      "Topic 60:\n",
      "let 25 way drive world said want don state course\n",
      "Topic 61:\n",
      "g9v a86 state come way b8f don second year right\n",
      "Topic 62:\n",
      "better ve like does problem different time things got just\n",
      "Topic 63:\n",
      "let need sure don time doesn day really tell right\n",
      "Topic 64:\n",
      "12 did probably power work state use things case let\n",
      "Topic 65:\n",
      "b8f really tell jesus data different world way mail ve\n",
      "Topic 66:\n",
      "25 new like 15 just year way don information best\n",
      "Topic 67:\n",
      "little work like just things tell different bit really know\n",
      "Topic 68:\n",
      "using information 20 space windows ll make time year lot\n",
      "Topic 69:\n",
      "mail good space 25 time way tell really work try\n",
      "Topic 70:\n",
      "bit 10 look drive want 15 didn sure things mail\n",
      "Topic 71:\n",
      "year did years think long just better second case like\n",
      "Topic 72:\n",
      "com like tell need mail 14 true day sure world\n",
      "Topic 73:\n",
      "government time god 00 available set windows thanks far problem\n",
      "Topic 74:\n",
      "set right 20 g9v sure better didn case just 00\n",
      "Topic 75:\n",
      "use used key need like does run true work using\n",
      "Topic 76:\n",
      "25 available government 14 ve just believe mr day run\n",
      "Topic 77:\n",
      "used program far does state doesn available sure using power\n",
      "Topic 78:\n",
      "key lot probably set com drive don sure 10 government\n",
      "Topic 79:\n",
      "really try day run just long say does true tell\n",
      "Topic 80:\n",
      "thanks work need like tell does run help new just\n",
      "Topic 81:\n",
      "doesn lot like just does tell true don problem good\n",
      "Topic 82:\n",
      "mr case 10 used does set didn g9v time program\n",
      "Topic 83:\n",
      "does jesus like say way come think don time things\n",
      "Topic 84:\n",
      "number way ll 12 bit people max government don law\n",
      "Topic 85:\n",
      "edu like need mail new use world state list does\n",
      "Topic 86:\n",
      "available 10 does really ax mr don need 16 far\n",
      "Topic 87:\n",
      "number long like new tell second 20 does years things\n",
      "Topic 88:\n",
      "world come true tell fact way didn time different right\n",
      "Topic 89:\n",
      "people think just like know tell say don need true\n",
      "Topic 90:\n",
      "space program use long work 14 15 years new time\n",
      "Topic 91:\n",
      "government say make like tell does did right years need\n",
      "Topic 92:\n",
      "mr space read program case look going just max let\n",
      "Topic 93:\n",
      "file use run windows available look like new different need\n",
      "Topic 94:\n",
      "15 20 16 14 max 25 12 true 10 new\n",
      "Topic 95:\n",
      "things thing like just say need does know tell true\n",
      "Topic 96:\n",
      "20 doesn try point 14 like does problem did data\n",
      "Topic 97:\n",
      "line run need second doesn say case right good don\n",
      "Topic 98:\n",
      "make lot bit want does com best fact better just\n",
      "Topic 99:\n",
      "different fact did point used tell just time say things\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, cv.get_feature_names_out(), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch: Use LDA w/ Gensim to do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
