{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a80143-0449-4359-b276-d8d5cddad43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481dceaa-0179-41da-a80b-83d69fc57f78",
   "metadata": {},
   "source": [
    "To implement Word2Vec, there are two flavors to choose from â€” Continuous Bag-Of-Words (CBOW) or continuous Skip-gram (SG). In short, CBOW attempts to guess the output (target word) from its neighbouring words (context words) whereas continuous Skip-Gram guesses the context words from a target word. Effectively, Word2Vec is based on distributional hypothesis where the context for each word is in its nearby words.\n",
    "\n",
    "* Skip-gram: works well with small amount of the training data, represents well even rare words or phrases\n",
    "* CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
    "\n",
    "To elaborate further, since Skip-gram learns to predict the context words from a given word, in case where two words (one appearing infrequently and the other more frequently) are placed side-by-side, both will have the same treatment when it comes to minimising loss since each word will be treated as both the target word and context word. Comparing that to CBOW, the infrequent word will only be part of a collection of context words used to predict the target word. Therefore, the model will assign the infrequent word a low probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794947e0-9c12-40dc-9d46-d6b994583fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "963768bc-ac37-4b1e-9e83-d6516e9fc877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "\n",
    "# Note the .lower() as upper and lowercase does not matter in our implementation\n",
    "# [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]\n",
    "corpus = [[word.lower() for word in text.split()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcd620c5-de31-4fce-ac4a-9acbf77e18aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'and',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'is',\n",
       "  'fun',\n",
       "  'and',\n",
       "  'exciting']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47bae219-382c-4fec-a481-c4b702d973c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperperameters\n",
    "settings = {\n",
    "\t'window_size': 2,\t# context window +- center word\n",
    "\t'n': 10,\t\t# dimensions of word embeddings, also refer to size of hidden layer\n",
    "\t'epochs': 50,\t\t# number of training epochs\n",
    "\t'learning_rate': 0.01\t# learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf6c4924-4f8d-4e83-b549-b1fe7266a5b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-09bb0885ca6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialise object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Numpy ndarray with one-hot representation for [target_word, context_words]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Word2Vec' object is not callable"
     ]
    }
   ],
   "source": [
    "# # Initialise object\n",
    "# w2v = word2vec()\n",
    "# # Numpy ndarray with one-hot representation for [target_word, context_words]\n",
    "# training_data = w2v.generate_training_data(settings, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68238bd8-736c-4f34-bc0c-d13340aac6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly initialise\n",
    "getW1 = [[0.236, -0.962, 0.686, 0.785, -0.454, -0.833, -0.744, 0.677, -0.427, -0.066],\n",
    "[-0.907, 0.894, 0.225, 0.673, -0.579, -0.428, 0.685, 0.973, -0.070, -0.811],\n",
    "[-0.576, 0.658, -0.582, -0.112, 0.662, 0.051, -0.401, -0.921, -0.158, 0.529],\n",
    "[0.517, 0.436, 0.092, -0.835, -0.444, -0.905, 0.879, 0.303, 0.332, -0.275],\n",
    "[0.859, -0.890, 0.651, 0.185, -0.511, -0.456, 0.377, -0.274, 0.182, -0.237],\n",
    "[0.368, -0.867, -0.301, -0.222, 0.630, 0.808, 0.088, -0.902, -0.450, -0.408],\n",
    "[0.728, 0.277, 0.439, 0.138, -0.943, -0.409, 0.687, -0.215, -0.807, 0.612],\n",
    "[0.593, -0.699, 0.020, 0.142, -0.638, -0.633, 0.344, 0.868, 0.913, 0.429],\n",
    "[0.447, -0.810, -0.061, -0.495, 0.794, -0.064, -0.817, -0.408, -0.286, 0.149]]\n",
    "\n",
    "getW2 = [[-0.868, -0.406, -0.288, -0.016, -0.560, 0.179, 0.099, 0.438, -0.551],\n",
    "[-0.395, 0.890, 0.685, -0.329, 0.218, -0.852, -0.919, 0.665, 0.968],\n",
    "[-0.128, 0.685, -0.828, 0.709, -0.420, 0.057, -0.212, 0.728, -0.690],\n",
    "\t\t[0.881, 0.238, 0.018, 0.622, 0.936, -0.442, 0.936, 0.586, -0.020],\n",
    "\t\t[-0.478, 0.240, 0.820, -0.731, 0.260, -0.989, -0.626, 0.796, -0.599],\n",
    "\t\t[0.679, 0.721, -0.111, 0.083, -0.738, 0.227, 0.560, 0.929, 0.017],\n",
    "\t\t[-0.690, 0.907, 0.464, -0.022, -0.005, -0.004, -0.425, 0.299, 0.757],\n",
    "\t\t[-0.054, 0.397, -0.017, -0.563, -0.551, 0.465, -0.596, -0.413, -0.395],\n",
    "\t\t[-0.838, 0.053, -0.160, -0.164, -0.671, 0.140, -0.149, 0.708, 0.425],\n",
    "\t\t[0.096, -0.995, -0.313, 0.881, -0.402, -0.631, -0.660, 0.184, 0.487]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94cf92fe-4666-4aaf-8f2d-1fb3f5826cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.868, -0.406, -0.288, -0.016, -0.56, 0.179, 0.099, 0.438, -0.551],\n",
       " [-0.395, 0.89, 0.685, -0.329, 0.218, -0.852, -0.919, 0.665, 0.968],\n",
       " [-0.128, 0.685, -0.828, 0.709, -0.42, 0.057, -0.212, 0.728, -0.69],\n",
       " [0.881, 0.238, 0.018, 0.622, 0.936, -0.442, 0.936, 0.586, -0.02],\n",
       " [-0.478, 0.24, 0.82, -0.731, 0.26, -0.989, -0.626, 0.796, -0.599],\n",
       " [0.679, 0.721, -0.111, 0.083, -0.738, 0.227, 0.56, 0.929, 0.017],\n",
       " [-0.69, 0.907, 0.464, -0.022, -0.005, -0.004, -0.425, 0.299, 0.757],\n",
       " [-0.054, 0.397, -0.017, -0.563, -0.551, 0.465, -0.596, -0.413, -0.395],\n",
       " [-0.838, 0.053, -0.16, -0.164, -0.671, 0.14, -0.149, 0.708, 0.425],\n",
       " [0.096, -0.995, -0.313, 0.881, -0.402, -0.631, -0.66, 0.184, 0.487]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9899c9-0ac1-48e2-9c69-07d36ed4f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.n = settings['n']\n",
    "\t\tself.lr = settings['learning_rate']\n",
    "\t\tself.epochs = settings['epochs']\n",
    "\t\tself.window = settings['window_size']\n",
    "\n",
    "\tdef generate_training_data(self, settings, corpus):\n",
    "\t\t# Find unique word counts using dictonary\n",
    "\t\tword_counts = defaultdict(int)\n",
    "\t\tfor row in corpus:\n",
    "\t\t\tfor word in row:\n",
    "\t\t\t\tword_counts[word] += 1\n",
    "\t\t#########################################################################################################################################################\n",
    "\t\t# print(word_counts)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "\t\t# # defaultdict(<class 'int'>, {'natural': 1, 'language': 1, 'processing': 1, 'and': 2, 'machine': 1, 'learning': 1, 'is': 1, 'fun': 1, 'exciting': 1})\t#\n",
    "\t\t#########################################################################################################################################################\n",
    "\n",
    "\t\t## How many unique words in vocab? 9\n",
    "\t\tself.v_count = len(word_counts.keys())\n",
    "\t\t#########################\n",
    "\t\t# print(self.v_count)\t#\n",
    "\t\t# 9\t\t\t\t\t\t#\n",
    "\t\t#########################\n",
    "\n",
    "\t\t# Generate Lookup Dictionaries (vocab)\n",
    "\t\tself.words_list = list(word_counts.keys())\n",
    "\t\t#################################################################################################\n",
    "\t\t# print(self.words_list)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "\t\t# ['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'exciting']\t#\n",
    "\t\t#################################################################################################\n",
    "\t\t\n",
    "\t\t# Generate word:index\n",
    "\t\tself.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "\t\t#############################################################################################################################\n",
    "\t\t# print(self.word_index)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "\t\t# # {'natural': 0, 'language': 1, 'processing': 2, 'and': 3, 'machine': 4, 'learning': 5, 'is': 6, 'fun': 7, 'exciting': 8}\t#\n",
    "\t\t#############################################################################################################################\n",
    "\n",
    "\t\t# Generate index:word\n",
    "\t\tself.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "\t\t#############################################################################################################################\n",
    "\t\t# print(self.index_word)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "\t\t# {0: 'natural', 1: 'language', 2: 'processing', 3: 'and', 4: 'machine', 5: 'learning', 6: 'is', 7: 'fun', 8: 'exciting'}\t#\n",
    "\t\t#############################################################################################################################\n",
    "\n",
    "\t\ttraining_data = []\n",
    "\n",
    "\t\t# Cycle through each sentence in corpus\n",
    "\t\tfor sentence in corpus:\n",
    "\t\t\tsent_len = len(sentence)\n",
    "\n",
    "\t\t\t# Cycle through each word in sentence\n",
    "\t\t\tfor i, word in enumerate(sentence):\n",
    "\t\t\t\t# Convert target word to one-hot\n",
    "\t\t\t\tw_target = self.word2onehot(sentence[i])\n",
    "\n",
    "\t\t\t\t# Cycle through context window\n",
    "\t\t\t\tw_context = []\n",
    "\n",
    "\t\t\t\t# Note: window_size 2 will have range of 5 values\n",
    "\t\t\t\tfor j in range(i - self.window, i + self.window+1):\n",
    "\t\t\t\t\t# Criteria for context word \n",
    "\t\t\t\t\t# 1. Target word cannot be context word (j != i)\n",
    "\t\t\t\t\t# 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
    "\t\t\t\t\t# 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
    "\t\t\t\t\tif j != i and j <= sent_len-1 and j >= 0:\n",
    "\t\t\t\t\t\t# Append the one-hot representation of word to w_context\n",
    "\t\t\t\t\t\tw_context.append(self.word2onehot(sentence[j]))\n",
    "\t\t\t\t\t\t# print(sentence[i], sentence[j]) \n",
    "\t\t\t\t\t\t#########################\n",
    "\t\t\t\t\t\t# Example:\t\t\t\t#\n",
    "\t\t\t\t\t\t# natural language\t\t#\n",
    "\t\t\t\t\t\t# natural processing\t#\n",
    "\t\t\t\t\t\t# language natural\t\t#\n",
    "\t\t\t\t\t\t# language processing\t#\n",
    "\t\t\t\t\t\t# language append \t\t#\n",
    "\t\t\t\t\t\t#########################\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t# training_data contains a one-hot representation of the target word and context words\n",
    "\t\t\t\t#################################################################################################\n",
    "\t\t\t\t# Example:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "\t\t\t\t# [Target] natural, [Context] language, [Context] processing\t\t\t\t\t\t\t\t\t#\n",
    "\t\t\t\t# print(training_data)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "\t\t\t\t# [[[1, 0, 0, 0, 0, 0, 0, 0, 0], [[0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]]]]\t#\n",
    "\t\t\t\t#################################################################################################\n",
    "\t\t\t\ttraining_data.append([w_target, w_context])\n",
    "\n",
    "\t\treturn np.array(training_data)\n",
    "\n",
    "\tdef word2onehot(self, word):\n",
    "\t\t# word_vec - initialise a blank vector\n",
    "\t\tword_vec = [0 for i in range(0, self.v_count)] # Alternative - np.zeros(self.v_count)\n",
    "\t\t#############################\n",
    "\t\t# print(word_vec)\t\t\t#\n",
    "\t\t# [0, 0, 0, 0, 0, 0, 0, 0]\t#\n",
    "\t\t#############################\n",
    "\n",
    "\t\t# Get ID of word from word_index\n",
    "\t\tword_index = self.word_index[word]\n",
    "\n",
    "\t\t# Change value from 0 to 1 according to ID of the word\n",
    "\t\tword_vec[word_index] = 1\n",
    "\n",
    "\t\treturn word_vec\n",
    "\n",
    "\tdef train(self, training_data):\n",
    "\t\t# Initialising weight matrices\n",
    "\t\t# np.random.uniform(HIGH, LOW, OUTPUT_SHAPE)\n",
    "\t\t# https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.uniform.html\n",
    "\t\tself.w1 = np.array(getW1)\n",
    "\t\tself.w2 = np.array(getW2)\n",
    "\t\t# self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
    "\t\t# self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
    "\t\t\n",
    "\t\t# Cycle through each epoch\n",
    "\t\tfor i in range(self.epochs):\n",
    "\t\t\t# Intialise loss to 0\n",
    "\t\t\tself.loss = 0\n",
    "\t\t\t# Cycle through each training sample\n",
    "\t\t\t# w_t = vector for target word, w_c = vectors for context words\n",
    "\t\t\tfor w_t, w_c in training_data:\n",
    "\t\t\t\t# Forward pass\n",
    "\t\t\t\t# 1. predicted y using softmax (y_pred) 2. matrix of hidden layer (h) 3. output layer before softmax (u)\n",
    "\t\t\t\ty_pred, h, u = self.forward_pass(w_t)\n",
    "\t\t\t\t#########################################\n",
    "\t\t\t\t# print(\"Vector for target word:\", w_t)\t#\n",
    "\t\t\t\t# print(\"W1-before backprop\", self.w1)\t#\n",
    "\t\t\t\t# print(\"W2-before backprop\", self.w2)\t#\n",
    "\t\t\t\t#########################################\n",
    "\n",
    "\t\t\t\t# Calculate error\n",
    "\t\t\t\t# 1. For a target word, calculate difference between y_pred and each of the context words\n",
    "\t\t\t\t# 2. Sum up the differences using np.sum to give us the error for this particular target word\n",
    "\t\t\t\tEI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
    "\t\t\t\t#########################\n",
    "\t\t\t\t# print(\"Error\", EI)\t#\n",
    "\t\t\t\t#########################\n",
    "\n",
    "\t\t\t\t# Backpropagation\n",
    "\t\t\t\t# We use SGD to backpropagate errors - calculate loss on the output layer \n",
    "\t\t\t\tself.backprop(EI, h, w_t)\n",
    "\t\t\t\t#########################################\n",
    "\t\t\t\t#print(\"W1-after backprop\", self.w1)\t#\n",
    "\t\t\t\t#print(\"W2-after backprop\", self.w2)\t#\n",
    "\t\t\t\t#########################################\n",
    "\n",
    "\t\t\t\t# Calculate loss\n",
    "\t\t\t\t# There are 2 parts to the loss function\n",
    "\t\t\t\t# Part 1: -ve sum of all the output +\n",
    "\t\t\t\t# Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)\n",
    "\t\t\t\t# Note: word.index(1) returns the index in the context word vector with value 1\n",
    "\t\t\t\t# Note: u[word.index(1)] returns the value of the output layer before softmax\n",
    "\t\t\t\tself.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "\t\t\t\t\n",
    "\t\t\t\t#############################################################\n",
    "\t\t\t\t# Break if you want to see weights after first target word \t#\n",
    "\t\t\t\t# break \t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "\t\t\t\t#############################################################\n",
    "\t\t\tprint('Epoch:', i, \"Loss:\", self.loss)\n",
    "\n",
    "\tdef forward_pass(self, x):\n",
    "\t\t# x is one-hot vector for target word, shape - 9x1\n",
    "\t\t# Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1\n",
    "\t\th = np.dot(x, self.w1)\n",
    "\t\t# Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1\n",
    "\t\tu = np.dot(h, self.w2)\n",
    "\t\t# Run 1x9 through softmax to force each element to range of [0, 1] - 1x8\n",
    "\t\ty_c = self.softmax(u)\n",
    "\t\treturn y_c, h, u\n",
    "\n",
    "\tdef softmax(self, x):\n",
    "\t\te_x = np.exp(x - np.max(x))\n",
    "\t\treturn e_x / e_x.sum(axis=0)\n",
    "\n",
    "\tdef backprop(self, e, h, x):\n",
    "\t\t# https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html\n",
    "\t\t# Column vector EI represents row-wise sum of prediction errors across each context word for the current center word\n",
    "\t\t# Going backwards, we need to take derivative of E with respect of w2\n",
    "\t\t# h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9\n",
    "\t\t# x - shape 9x1, w2 - 10x9, e.T - 9x1\n",
    "\t\tdl_dw2 = np.outer(h, e)\n",
    "\t\tdl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "\t\t########################################\n",
    "\t\t# print('Delta for w2', dl_dw2)\t\t\t#\n",
    "\t\t# print('Hidden layer', h)\t\t\t\t#\n",
    "\t\t# print('np.dot', np.dot(self.w2, e.T))\t#\n",
    "\t\t# print('Delta for w1', dl_dw1)\t\t\t#\n",
    "\t\t#########################################\n",
    "\n",
    "\t\t# Update weights\n",
    "\t\tself.w1 = self.w1 - (self.lr * dl_dw1)\n",
    "\t\tself.w2 = self.w2 - (self.lr * dl_dw2)\n",
    "\n",
    "\t# Get vector from word\n",
    "\tdef word_vec(self, word):\n",
    "\t\tw_index = self.word_index[word]\n",
    "\t\tv_w = self.w1[w_index]\n",
    "\t\treturn v_w\n",
    "\n",
    "\t# Input vector, returns nearest word(s)\n",
    "\tdef vec_sim(self, word, top_n):\n",
    "\t\tv_w1 = self.word_vec(word)\n",
    "\t\tword_sim = {}\n",
    "\n",
    "\t\tfor i in range(self.v_count):\n",
    "\t\t\t# Find the similary score for each word in vocab\n",
    "\t\t\tv_w2 = self.w1[i]\n",
    "\t\t\ttheta_sum = np.dot(v_w1, v_w2)\n",
    "\t\t\ttheta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "\t\t\ttheta = theta_sum / theta_den\n",
    "\n",
    "\t\t\tword = self.index_word[i]\n",
    "\t\t\tword_sim[word] = theta\n",
    "\n",
    "\t\twords_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "\t\tfor word, sim in words_sorted[:top_n]:\n",
    "\t\t\tprint(word, sim)\n",
    "\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fe1acc-3733-452f-8a1b-29ff7b5461d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 81.99625348926826\n",
      "Epoch: 1 Loss: 80.37706194534589\n",
      "Epoch: 2 Loss: 78.9116663644708\n",
      "Epoch: 3 Loss: 77.57806136236584\n",
      "Epoch: 4 Loss: 76.35803881618284\n",
      "Epoch: 5 Loss: 75.23652831371102\n",
      "Epoch: 6 Loss: 74.20101998340208\n",
      "Epoch: 7 Loss: 73.24108053436369\n",
      "Epoch: 8 Loss: 72.34795785304598\n",
      "Epoch: 9 Loss: 71.51426275735359\n",
      "Epoch: 10 Loss: 70.73371481522524\n",
      "Epoch: 11 Loss: 70.00093990321889\n",
      "Epoch: 12 Loss: 69.31130890283573\n",
      "Epoch: 13 Loss: 68.66080883957898\n",
      "Epoch: 14 Loss: 68.04593952257905\n",
      "Epoch: 15 Loss: 67.46363022458323\n",
      "Epoch: 16 Loss: 66.91117214136592\n",
      "Epoch: 17 Loss: 66.38616331680674\n",
      "Epoch: 18 Loss: 65.88646345805446\n",
      "Epoch: 19 Loss: 65.41015663657106\n",
      "Epoch: 20 Loss: 64.95552031189413\n",
      "Epoch: 21 Loss: 64.52099945530341\n",
      "Epoch: 22 Loss: 64.10518481350977\n",
      "Epoch: 23 Loss: 63.70679455597247\n",
      "Epoch: 24 Loss: 63.32465870722654\n",
      "Epoch: 25 Loss: 62.95770588813032\n",
      "Epoch: 26 Loss: 62.60495198520285\n",
      "Epoch: 27 Loss: 62.26549044132181\n",
      "Epoch: 28 Loss: 61.93848391870793\n",
      "Epoch: 29 Loss: 61.62315712999325\n",
      "Epoch: 30 Loss: 61.318790668129665\n",
      "Epoch: 31 Loss: 61.02471569320668\n",
      "Epoch: 32 Loss: 60.74030935570918\n",
      "Epoch: 33 Loss: 60.464990852779636\n",
      "Epoch: 34 Loss: 60.19821802777383\n",
      "Epoch: 35 Loss: 59.939484434684516\n",
      "Epoch: 36 Loss: 59.688316798517064\n",
      "Epoch: 37 Loss: 59.44427281092188\n",
      "Epoch: 38 Loss: 59.20693920766953\n",
      "Epoch: 39 Loss: 58.975930081127764\n",
      "Epoch: 40 Loss: 58.75088538690976\n",
      "Epoch: 41 Loss: 58.53146960938955\n",
      "Epoch: 42 Loss: 58.317370555858474\n",
      "Epoch: 43 Loss: 58.108298253732954\n",
      "Epoch: 44 Loss: 57.903983929414984\n",
      "Epoch: 45 Loss: 57.70417905114165\n",
      "Epoch: 46 Loss: 57.508654421439005\n",
      "Epoch: 47 Loss: 57.3171993076209\n",
      "Epoch: 48 Loss: 57.129620601161825\n",
      "Epoch: 49 Loss: 56.94574199875216\n",
      "machine [ 0.76702922 -0.95673743  0.49207258  0.16240808 -0.4538815  -0.74678226\n",
      "  0.42072706 -0.04147312  0.08947326 -0.24245257]\n",
      "machine 0.9999999999999999\n",
      "fun 0.6223490454018771\n",
      "and 0.5190154215400251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-d904d7a90e7b>:90: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(training_data)\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "\t'window_size': 2,\t\t\t# context window +- center word\n",
    "\t'n': 10,\t\t\t\t\t# dimensions of word embeddings, also refer to size of hidden layer\n",
    "\t'epochs': 50,\t\t\t\t# number of training epochs\n",
    "\t'learning_rate': 0.01\t\t# learning rate\n",
    "}\n",
    "\n",
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "\n",
    "# Note the .lower() as upper and lowercase does not matter in our implementation\n",
    "# [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "\n",
    "# Initialise object\n",
    "w2v = word2vec()\n",
    "\n",
    "# Numpy ndarray with one-hot representation for [target_word, context_words]\n",
    "training_data = w2v.generate_training_data(settings, corpus)\n",
    "\n",
    "# Training\n",
    "w2v.train(training_data)\n",
    "\n",
    "# Get vector for word\n",
    "word = \"machine\"\n",
    "vec = w2v.word_vec(word)\n",
    "print(word, vec)\n",
    "\n",
    "# Find similar words\n",
    "w2v.vec_sim(\"machine\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814a2c9-d98a-42f8-83bc-d3e9c58475ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b1bcd-5bcb-4d8c-b449-78e92a53d831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7824cecc-6cb5-4d62-98c0-5abd05e43546",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b5d92f0-0ac0-4c0f-887b-dced93d50922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/lilakelland/opt/anaconda3/lib/python3.8/site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/lilakelland/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf1b51e4-6d85-4bf4-ad20-326ce97b5b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/lilakelland/opt/anaconda3/lib/python3.8/site-packages (4.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cd7ab-7f51-484a-8cd6-a340ae3b3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a05660b-fc3f-4131-a3dd-7482e71185a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The article we are going to scrape is the Wikipedia article on Artificial Intelligence. \n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scrapped_data .read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf68f4ee-f864-40e3-baa9-d8d6e71cc8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaing the text\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec777d80-1e69-471a-9abd-f68f2a0fa057",
   "metadata": {},
   "source": [
    "Creating word2Vec model\n",
    "\n",
    "With Gensim, it is extremely straightforward to create Word2Vec model. The word list is passed to the Word2Vec class of the gensim.models package. We need to specify the value for the min_count parameter. A value of 2 for min_count specifies to include only those words in the Word2Vec model that appear at least twice in the corpus. The following script creates Word2Vec model using the Wikipedia article we scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c65eca7-ad81-42a9-b95c-a297c64c51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8387f-ef1f-4c0e-a698-649c074fc5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6ccac92-2a17-4e9a-a583-b7c03b1899e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f8da95cd9c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp_env/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.vocab\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db4242e8-a8e8-426a-81f0-b7d1cafed668",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7de7b8b-5634-415a-9c33-61879417a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('century', 0.3898581266403198),\n",
       " ('ai', 0.359462708234787),\n",
       " ('machines', 0.35074132680892944),\n",
       " ('solving', 0.32108262181282043),\n",
       " ('field', 0.31676924228668213),\n",
       " ('inspired', 0.31561052799224854),\n",
       " ('easy', 0.3016696572303772),\n",
       " ('problem', 0.3004421889781952),\n",
       " ('widely', 0.29972344636917114),\n",
       " ('even', 0.29903778433799744)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words = word2vec.wv.most_similar('intelligence')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a009cd-714d-4125-b71b-8b25237a197c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
