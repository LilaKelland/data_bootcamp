{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106b4839-122b-4776-8b43-4602ea076dd4",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "Find the unique words i.e., vocabulary from the list of documents. Parse each document word with the vocabulary, if present ‘1’ else ‘0’. This makes each document vector maintain the same length that of vocabulary length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6fc988-77b9-4c00-ba35-d0f88e44cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['this', 'hate', 'superb,', 'am', 'i', 'in', 'love', 'phone']\n",
      "vectors:  [[1, 0, 1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 1, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \"I hate this phone\"]\n",
    "words = list(set([word for doc in docs for word in doc.lower().split()]))\n",
    "vectors = []\n",
    "for doc in docs:\n",
    "    vectors.append([1 if word in doc.lower().split() else 0 for word in words])\n",
    "print(\"vocabulary: \", words)   \n",
    "print(\"vectors: \", vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc29fae-2268-43d8-bc6f-ca8429408cab",
   "metadata": {},
   "source": [
    "### Word Counts with CountVectorizer(scikit-learn) \n",
    "Tokenize the collection of documents and form a vocabulary with it and use this vocabulary to encode new documents. We can use CountVectorizer of the scikit-learn library. It by default remove punctuation and lower the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba49a73-318b-468b-b1ef-4cbb11676ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\n",
      "shape:  (2, 7)\n",
      "vectors:  [[1 0 2 1 1 1 1]\n",
      " [0 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# list of documents\n",
    "docs = ['SUPERB, I AM IN LOVE IN THIS PHONE', 'I hate this phone']\n",
    "\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(docs)\n",
    "\n",
    "# summarize encoded vector\n",
    "print('shape: ', vector.shape)\n",
    "print('vectors: ', vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173295c-0a3f-4eb8-a46e-8e7c8214d764",
   "metadata": {},
   "source": [
    " It will make sure the word present in the vocabulary and if present it prints the number of occurrences of the word in the vocabulary. The word “in” present twice in the first document so ‘2’ appeared in the first vector.\n",
    " \n",
    "### TF-IDF is a popular method.\n",
    "Acronym is “Term Frequency and Inverse Document Frequency”. TF-IDF is word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
    "There are a few types of weighting schemes for tf-idf in general.\n",
    "\n",
    "tf-idf = tf * (idf + 1) = tf + tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9291af-3763-4ee2-b7d7-316f3efc7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\n",
      "idfs:  [1.40546511 1.40546511 1.40546511 1.40546511 1.         1.40546511\n",
      " 1.        ]\n",
      "vectors:  [[0.35327777 0.         0.70655553 0.35327777 0.25136004 0.35327777\n",
      "  0.25136004]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of documents\n",
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \n",
    "        \"I hate this phone\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "# summarize\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "print('idfs: ', vectorizer.idf_)\n",
    "\n",
    "# encode document (I think this normalizes as well? - sums of squares = 1)\n",
    "vector = vectorizer.transform([docs[0]])\n",
    "\n",
    "# summarize encoded vector\n",
    "print('vectors: ', vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51b21b-a683-485f-aa89-e5f4fe977237",
   "metadata": {},
   "source": [
    "Example: Let's take the word “phone”, it presents in both documents, so log(1+2/1+2) + 1 == log(3/3) + 1 == 1. idfs[4] is 1, you can check in the above snippet.\n",
    "Let's take another word “love”, it presents in one document, so log(1+2/1+1) + 1 = log(3/2) + 1 == 1.40546510811. idfs[3] is 1.40546511.\n",
    "\n",
    "The final step is vector normalization, scikit-learn uses ‘l2’ normalization technique for each document.\n",
    "The sum of squares of each value in the document vector is always 1.\n",
    "\n",
    "Tf-idf is the best vectorization method among these three because it prioritises the words in each document. IDF value for the word “this” is less since it present in both the documents. So, unlike word counts which give a higher value for stop words like “in”, “this”, word frequency lowers the value if it present in more number of documents, because stop words repeat in each document almost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0c382-6bdf-455a-85e2-44ee15373e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
