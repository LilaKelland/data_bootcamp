{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af0d898-7f0f-4623-bae2-ac148d0f9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to clean data : splitting it into words, handling puncuation\n",
    "\n",
    "# https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f2b16e3-b09d-4ad6-ab29-d6549cdee9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc2c20-6523-4cf4-b4e2-c20c4ae777cb",
   "metadata": {},
   "source": [
    "### 2. Split by Whitespace\n",
    "Clean text often means a list of words or tokens that we can work with in our machine learning models.\n",
    "\n",
    "This means converting the raw text into a list of words and saving it again.\n",
    "\n",
    "A very simple way to do this would be to split the document by white space, including ” “, new lines, tabs and more. We can do this in Python with the split() function on the loaded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad495082-3d17-49b8-a56f-db6d1661eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '“What’s', 'happened', 'to', 'me?”', 'he', 'thought.', 'It', 'wasn’t', 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# split words\n",
    "\n",
    "# load text\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])\n",
    "\n",
    "# punctuation preserved “wasn’t” and “armour-like“ (good), but period kept with word (not good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85998d27-0221-40ad-8146-5520354c015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']\n"
     ]
    }
   ],
   "source": [
    "# Select just words with re\n",
    "\n",
    "# load text\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split based on words only\n",
    "import re\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])\n",
    "\n",
    "# “What’s” is also two words “What” and “s” (not great)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7952a9-63e5-4df7-b89e-18163cf312dd",
   "metadata": {},
   "source": [
    "### 3. Split by Whitespace and Remove Punctuation\n",
    "We may want the words, but without the punctuation like commas and quotes. We also want to keep contractions together.\n",
    "\n",
    "“2. Split by Whitespace“), then use string translation to replace all punctuation with nothing (e.g. remove it).\n",
    "\n",
    "Python provides a constant called string.punctuation that provides a great list of punctuation characters. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30bfb319-177d-40ab-9649-3e2feba20d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36575e-c4bd-42f8-a022-af902d1a4d75",
   "metadata": {},
   "source": [
    "Python offers a function called translate() that will map one set of characters to another.\n",
    "\n",
    "We can use the function maketrans() to create a mapping table. We can create an empty mapping table, but the third argument of this function allows us to list all of the characters to remove during the translation process. For example:\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19346f2-b1a6-423f-bf8a-aa65a643d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19b03b60-d677-446d-84cd-e2bf1131b0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '“What’s', 'happened', 'to', 'me”', 'he', 'thought', 'It', 'wasn’t', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in words]\n",
    "print(stripped[:100])\n",
    "\n",
    "# Contractions like “What’s” have become “Whats” but “armour-like” has become “armourlike“."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ea542-c559-4e12-9a53-ee77b363c54d",
   "metadata": {},
   "source": [
    "### 4. Normalizing Case\n",
    "It is common to convert all words to one case.\n",
    "\n",
    "This means that the vocabulary will shrink in size, but some distinctions are lost (e.g. “Apple” the company vs “apple” the fruit is a commonly used example).\n",
    "\n",
    "We can convert all words to lowercase by calling the lower() function on each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7be8dca4-fbab-4999-ac30-44350f9fab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '“what’s', 'happened', 'to', 'me?”', 'he', 'thought.', 'it', 'wasn’t', 'a', 'dream.', 'his', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e902bc-4765-4456-a6ba-42fed1382de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning is hard and full of trade offs - simple is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a658070-b8b3-4e27-8f78-98a0fa9ef32f",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning with NLTK\n",
    "The Natural Language Toolkit, or NLTK for short,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06bf52ee-c798-4354-ae42-5378212cf057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lilakelland/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lilakelland/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1747e2-a61d-4959-9336-dca56074dce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231397a-7355-4d32-b563-6be9d4507c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or from command line\n",
    "# python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491a05e-09fb-4768-890e-0da9a73697f5",
   "metadata": {},
   "source": [
    "### Split into Sentences\n",
    "Some modelling tasks prefer input be in form sentences or paragraphs (word2vec). You could first split your text into sentences, split each sentence into words, then save each sentence to file, one per line.\n",
    "\n",
    "NLTK provides the sent_tokenize() function to split text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8af4076a-d8d1-42ae-a52a-87574f9b3a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "# Splits into sentences and prints first one out\n",
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into sentences\n",
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47e1bc-4724-4517-b538-f02e073b10c2",
   "metadata": {},
   "source": [
    "### Split into Words\n",
    "NLTK has function word_tokenize() - for splitting strings into tokens (workds).\n",
    "\n",
    "Uses white space and punctuation. For example, commas and periods are taken as separate tokens. Contractions are split apart (e.g. “What’s” becomes “What” “‘s“). Quotes are kept, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77aba4fc-6cbc-4843-8fb7-a06d29271d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '“', 'What', '’', 's', 'happened']\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b24ee-82d9-4e22-9215-c7f5e68e09ff",
   "metadata": {},
   "source": [
    "### Filter Out Punctuation\n",
    "\n",
    "Do this by iterating over all tokesn and keeping those alphabetic (isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aa01193-8e8d-4dab-abb9-4792de3e8040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])\n",
    "\n",
    "# see that not only punctuation tokens, but examples like “armour-like” and “‘s” were also filtered out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc163f0-5971-4593-a5a8-e49c98db0a5e",
   "metadata": {},
   "source": [
    "### Filter Out Stop Words (and Pipeline)\n",
    "(words that give no deeper meaning to phrase - the, a is) For some applications, it may make sense to remove these,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "256938a4-0134-49c0-8f60-a7f91d769d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "\n",
    "# note all lower case and all have punctuation removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978689a-f3ac-495c-b4cd-21fc3a109ab8",
   "metadata": {},
   "source": [
    "#### Pipeline Data Prep\n",
    "1. Load the raw text.\n",
    "2. Split into tokens.\n",
    "3. Convert to lowercase.\n",
    "4. Remove punctuation from each token.\n",
    "5. Filter out remaining tokens that are not alphabetic.\n",
    "6. Filter out tokens that are stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8fe6235-9427-478e-8178-8795c13f9dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'travelling', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer', 'gregor', 'turned', 'look', 'window']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8418c-80a8-4059-8c09-025d3a174360",
   "metadata": {},
   "source": [
    "### Stem Words\n",
    "stemmig refers to process of reducing each word to root or base.\n",
    "* (fishing, fished, fisher - fish)\n",
    "\n",
    "This helps reduce vocabulary and helps with sense or sentiment of document rather than deep meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef4573e7-73de-4832-bfec-6700defe4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'he', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as', 'he', 'look', '.', '“', 'what', '’', 's', 'happen']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'data/metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])\n",
    "\n",
    "# note trouble has become troubl - also converted to lowercase for internal lookups\n",
    "\n",
    "# Also a library for lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c84d4-c3c2-487a-abda-7e4624f35739",
   "metadata": {},
   "source": [
    "#### Here is a short list of additional considerations when cleaning text:\n",
    "\n",
    "* Handling large documents and large collections of text documents that do not fit into memory.\n",
    "* Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "* Transliteration of characters from other languages into English.\n",
    "* Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "* Handling of domain specific words, phrases, and acronyms.\n",
    "* Handling or removing numbers, such as dates and amounts.\n",
    "* Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248047d-df84-4410-a319-91ecad6f6889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
