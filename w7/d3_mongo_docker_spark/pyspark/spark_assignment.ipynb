{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6a8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.guru99.com/pyspark-tutorial.html\n",
    "# but fixed so if want to follow - go to compass\n",
    "\n",
    "# Also see https://towardsdatascience.com/pyspark-data-manipulation-tutorial-8c62652f35fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28bce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the docker container we have downloaded in the Docker exercise using the following command.\n",
    "\n",
    "# docker run  --rm  -p 8888:8888  -v <YOUR WORKING DIR>:/home/jovyan/work/  jupyter/pyspark-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a349ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/27 23:20:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/10/27 23:20:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a493fe4",
   "metadata": {},
   "source": [
    "#### SQLContext\n",
    "Can use it to create DataFrame.\n",
    "Allows connecting the engine with different data sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085e1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b1ad1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = sqlContext.read.option(\"inferSchema\",True).option(\"header\",True).csv(\"titanic_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f6541",
   "metadata": {},
   "source": [
    "### Machine Learning with PySpark\n",
    "Following are the steps to build a Machine Learning program with PySpark:\n",
    "\n",
    "1. Basic operation with PySpark\n",
    "2. Data preprocessing\n",
    "3. Build a data processing pipeline\n",
    "4. Build the classifier: logistic\n",
    "5. Train and evaluate the model\n",
    "6. Tune the hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8650c",
   "metadata": {},
   "source": [
    "### 1. Basic operation with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba42a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at the data type\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4dd5ba0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_865/3820466740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Fare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "# df['Fare'].values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca52a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name                                               |Sex   |Age |SibSp|Parch|Ticket          |Fare   |Cabin|Embarked|\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|1          |0       |3     |Braund, Mr. Owen Harris                            |male  |22.0|1    |0    |A/5 21171       |7.25   |null |S       |\n",
      "|2          |1       |1     |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|female|38.0|1    |0    |PC 17599        |71.2833|C85  |C       |\n",
      "|3          |1       |3     |Heikkinen, Miss. Laina                             |female|26.0|0    |0    |STON/O2. 3101282|7.925  |null |S       |\n",
      "|4          |1       |1     |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |female|35.0|1    |0    |113803          |53.1   |C123 |S       |\n",
      "|5          |0       |3     |Allen, Mr. William Henry                           |male  |35.0|0    |0    |373450          |8.05   |null |S       |\n",
      "+-----------+--------+------+---------------------------------------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can see the data with show.\n",
    "\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b8a13",
   "metadata": {},
   "source": [
    "To convert the continuous variable in the right format, you can use recast the columns. You can use **withColumn** to tell Spark which column to operate the transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8642b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: float (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['age', 'Fare']\n",
    "\n",
    "# Convert the type\n",
    "df = convertColumn(df, CONTI_FEATURES, FloatType())\n",
    "\n",
    "# Check the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506ec30",
   "metadata": {},
   "source": [
    "### Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "623a96c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   Fare|\n",
      "+----+-------+\n",
      "|22.0|   7.25|\n",
      "|38.0|71.2833|\n",
      "|26.0|  7.925|\n",
      "|35.0|   53.1|\n",
      "|35.0|   8.05|\n",
      "+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','Fare').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff4e8b",
   "metadata": {},
   "source": [
    "### Count by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "529e1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can chain these\n",
    "# df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ea22c",
   "metadata": {},
   "source": [
    "### Describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3919e91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764704046|0.5230078563411896|0.38159371492704824|260318.54916792738|32.20420804114722| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332370992|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342916316158| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d739467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      capital_gain|\n",
      "+-------+------------------+\n",
      "|  count|             32561|\n",
      "|   mean|1077.6488437087312|\n",
      "| stddev| 7385.292084840329|\n",
      "|    min|               0.0|\n",
      "|    max|           99999.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if only want statistics of one column:\n",
    "df.describe('capital_gain').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f11ec0",
   "metadata": {},
   "source": [
    "### Crosstab computation\n",
    "In some occasions, it can be interesting to see the descriptive statistics between two pairwise columns. For instance, you can count the number of people with income below or above 50k by education level. This operation is called a crosstab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28b4204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+\n",
      "|age_label| <=50K| >50K|\n",
      "+---------+------+-----+\n",
      "|     17.0|   395|    0|\n",
      "|     18.0|   550|    0|\n",
      "|     19.0|   710|    2|\n",
      "|     20.0|   753|    0|\n",
      "|     21.0|   717|    3|\n",
      "|     22.0|   752|   13|\n",
      "|     23.0|   865|   12|\n",
      "|     24.0|   767|   31|\n",
      "|     25.0|   788|   53|\n",
      "|     26.0|   722|   63|\n",
      "|     27.0|   754|   81|\n",
      "|     28.0|   748|  119|\n",
      "|     29.0|   679|  134|\n",
      "|     30.0|   690|  171|\n",
      "|     31.0|   705|  183|\n",
      "|     32.0|   639|  189|\n",
      "|     33.0|   684|  191|\n",
      "|     34.0|   643|  243|\n",
      "|     35.0|   659|  217|\n",
      "|     36.0|   635|  263|\n",
      "+---------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crosstab('age', 'label').sort(\"age_label\").show()\n",
    "\n",
    "# You can see no people have revenue above 50k when they are young."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c7b05",
   "metadata": {},
   "source": [
    "### Drop column\n",
    "There are two intuitive commands to drop columns:\n",
    "\n",
    "* drop(): Drop a column\n",
    "* dropna(): Drop NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31d28abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'marital',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital_gain',\n",
       " 'capital_loss',\n",
       " 'hours_week',\n",
       " 'native_country',\n",
       " 'label']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('education_num').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c5054",
   "metadata": {},
   "source": [
    "### Filter data\n",
    "You can use filter() to apply descriptive statistics in a subset of data. For instance, you can count the number of people above 40:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35a8492c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13443"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.age > 40).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7dc960",
   "metadata": {},
   "source": [
    "### Descriptive statistics by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f5dbc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             marital| avg(capital_gain)|\n",
      "+--------------------+------------------+\n",
      "|             Widowed| 571.0715005035247|\n",
      "| Married-spouse-a...| 653.9832535885167|\n",
      "|   Married-AF-spouse| 432.6521739130435|\n",
      "|  Married-civ-spouse|1764.8595085470085|\n",
      "|            Divorced| 728.4148098131893|\n",
      "|       Never-married|376.58831788823363|\n",
      "|           Separated| 535.5687804878049|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('marital').agg({'capital_gain': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ecb524",
   "metadata": {},
   "source": [
    "## Step 2: Data preprocessing\n",
    "\n",
    "For instance, you know that age is not a linear function with the income. When people are young, their income is usually lower than mid-age. After retirement, a household uses their saving, meaning a decrease in income. To capture this pattern, you can add a square to the age feature\n",
    "\n",
    "### Add age square\n",
    "To add a new feature, you need to:\n",
    "\n",
    "* Select the column\n",
    "* Apply the transformation and add it to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9f99b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: float (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: float (nullable = true)\n",
      " |-- capital_loss: float (nullable = true)\n",
      " |-- hours_week: float (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- age_square: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# 1 Select the column\n",
    "age_square = df.select(col(\"age\")**2)\n",
    "\n",
    "# 2 Apply the transformation and add it to the DataFrame\n",
    "df = df.withColumn(\"age_square\", col(\"age\")**2)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b545d35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=39.0, age_square=1521.0, workclass=' State-gov', fnlwgt=77516.0, education=' Bachelors', education_num=13.0, marital=' Never-married', occupation=' Adm-clerical', relationship=' Not-in-family', race=' White', sex=' Male', capital_gain=2174.0, capital_loss=0.0, hours_week=40.0, native_country=' United-States', label=' <=50K')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change column order:\n",
    "COLUMNS = ['age', 'age_square', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "           'hours_week', 'native_country', 'label']\n",
    "df = df.select(COLUMNS)\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebf5e5",
   "metadata": {},
   "source": [
    "### Exclude Holand-Netherlands\n",
    "When a group within a feature has only one observation, it brings no information to the model. On the contrary, it can lead to an error during the cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4264a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native_country|count(native_country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|            Scotland|                   12|\n",
      "|            Honduras|                   13|\n",
      "|             Hungary|                   13|\n",
      "| Outlying-US(Guam...|                   14|\n",
      "|          Yugoslavia|                   16|\n",
      "|                Laos|                   18|\n",
      "|            Thailand|                   18|\n",
      "|            Cambodia|                   19|\n",
      "|     Trinadad&Tobago|                   19|\n",
      "|                Hong|                   20|\n",
      "|             Ireland|                   24|\n",
      "|             Ecuador|                   28|\n",
      "|              Greece|                   29|\n",
      "|              France|                   29|\n",
      "|                Peru|                   31|\n",
      "|           Nicaragua|                   34|\n",
      "|            Portugal|                   37|\n",
      "|                Iran|                   43|\n",
      "|               Haiti|                   44|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the origin of the household\n",
    "\n",
    "df.filter(df.native_country == 'Holand-Netherlands').count()\n",
    "df.groupby('native_country').agg({'native_country': 'count'}).sort(asc(\"count(native_country)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "098e1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature native_country has only one household coming from Netherland. We can exclude it.\n",
    "\n",
    "df_remove = df.filter(df.native_country != 'Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51bca82",
   "metadata": {},
   "source": [
    "## Step 3: Build a data processing pipeline\n",
    "Similar to scikit-learn, Pyspark has a pipeline API.\n",
    "\n",
    "The steps to transform the data are very similar to scikit-learn. You need to:\n",
    "\n",
    "* Index the string to numeric\n",
    "* Create the one hot encoder\n",
    "* Transform the data\n",
    "* Two APIs do the job: StringIndexer, OneHotEncoder\n",
    "\n",
    "1. First of all, you select the string column to index. The **inputCol** is the name of the column in the dataset. **outputCol** is the new name given to the transformed column.\n",
    "2. Fit the data and transform it\n",
    "3. Create the news columns based on the group. For instance, if there are 10 groups in the feature, the new matrix will have 10 columns, one for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c817c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "| age|age_square|        workclass| fnlwgt| education|education_num|            marital|      occupation|  relationship|  race|  sex|capital_gain|capital_loss|hours_week|native_country| label|workclass_encoded|workclass_vec|\n",
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "|39.0|    1521.0|        State-gov|77516.0| Bachelors|         13.0|      Never-married|    Adm-clerical| Not-in-family| White| Male|      2174.0|         0.0|      40.0| United-States| <=50K|              4.0|(9,[4],[1.0])|\n",
      "|50.0|    2500.0| Self-emp-not-inc|83311.0| Bachelors|         13.0| Married-civ-spouse| Exec-managerial|       Husband| White| Male|         0.0|         0.0|      13.0| United-States| <=50K|              1.0|(9,[1],[1.0])|\n",
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Example encoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"workclass_encoded\", outputCol=\"workclass_vec\").fit(indexed)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773a894",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "You will build a pipeline to convert all the precise features and add them to the final dataset. The pipeline will have four operations, but feel free to add as many operations as you want.\n",
    "\n",
    "1. Encode the categorical data\n",
    "2. Index the label feature\n",
    "3. Add continuous variable\n",
    "4. Assemble the steps.\n",
    "\n",
    "Each step is stored in a list named stages. This list will tell the VectorAssembler what operation to perform inside the pipeline.\n",
    "\n",
    "1. Encode the categorical data\n",
    "\n",
    "This step is very similar to the above example, except that you loop over all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd053531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexer_3c8c8d4e0202\n",
      "StringIndexer_f39ad1d15440\n",
      "StringIndexer_8188add96cbd\n",
      "StringIndexer_63d0c8058a5b\n",
      "StringIndexer_eb34314034fc\n",
      "StringIndexer_621c47d0cfd2\n",
      "StringIndexer_5d14730ed855\n",
      "StringIndexer_37e8ff5d2753\n",
      "[StringIndexer_3c8c8d4e0202, OneHotEncoder_4336bd16f9ff, StringIndexer_f39ad1d15440, OneHotEncoder_6be94051fa2e, StringIndexer_8188add96cbd, OneHotEncoder_f0d50dd4d00b, StringIndexer_63d0c8058a5b, OneHotEncoder_149dcd2f2e37, StringIndexer_eb34314034fc, OneHotEncoder_7c799f02c055, StringIndexer_621c47d0cfd2, OneHotEncoder_6b0047914bd2, StringIndexer_5d14730ed855, OneHotEncoder_893aeddd2290, StringIndexer_37e8ff5d2753, OneHotEncoder_784128f7793b]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    print(stringIndexer)\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "#     print(encoder)\n",
    "    stages += [stringIndexer, encoder]\n",
    "print(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fba69",
   "metadata": {},
   "source": [
    "2. Index the label feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac6e9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx =  StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c5e8b",
   "metadata": {},
   "source": [
    "3. Add continuous variable\n",
    "\n",
    "The inputCols of the VectorAssembler is a list of columns. You can create a new list containing all the new columns. The code below populate the list with encoded categorical features and continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a0aad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9e81d",
   "metadata": {},
   "source": [
    "4. Assemble the steps.\n",
    "\n",
    "Finally, you pass all the steps in the VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b334e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50528cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to pipeline\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88592aab",
   "metadata": {},
   "source": [
    "If you check the new dataset, you can see that it contains all the features, transformed and not transformed. You are only interested by the newlabel and features. The features includes all the transformed features and the continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85b2e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/27 22:14:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=39.0, age_square=1521.0, workclass=' State-gov', fnlwgt=77516.0, education=' Bachelors', education_num=13.0, marital=' Never-married', occupation=' Adm-clerical', relationship=' Not-in-family', race=' White', sex=' Male', capital_gain=2174.0, capital_loss=0.0, hours_week=40.0, native_country=' United-States', label=' <=50K', workclassIndex=4.0, workclassclassVec=SparseVector(8, {4: 1.0}), educationIndex=2.0, educationclassVec=SparseVector(15, {2: 1.0}), maritalIndex=1.0, maritalclassVec=SparseVector(6, {1: 1.0}), occupationIndex=3.0, occupationclassVec=SparseVector(14, {3: 1.0}), relationshipIndex=1.0, relationshipclassVec=SparseVector(5, {1: 1.0}), raceIndex=0.0, raceclassVec=SparseVector(4, {0: 1.0}), sexIndex=0.0, sexclassVec=SparseVector(1, {0: 1.0}), native_countryIndex=0.0, native_countryclassVec=SparseVector(41, {0: 1.0}), newlabel=0.0, features=SparseVector(100, {4: 1.0, 10: 1.0, 24: 1.0, 32: 1.0, 44: 1.0, 48: 1.0, 52: 1.0, 53: 1.0, 94: 39.0, 95: 77516.0, 96: 2174.0, 97: 13.0, 99: 40.0}))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6efd0a4",
   "metadata": {},
   "source": [
    "### Step 4 Build the Classifier: Logistic\n",
    "\n",
    "To make it go faster, convert to DenseVector type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6d3dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b25fde40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[0.0,1.0,0.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You are ready to create the train data as a DataFrame. You use the sqlContext\n",
    "\n",
    "df_train = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "df_train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f6b0e",
   "metadata": {},
   "source": [
    "### Create train/test set\n",
    "You split the dataset 80/20 with randomSplit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dd5fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1ef73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|       19841|\n",
      "|  1.0|        6299|\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:=============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let's count how many people with income below/above 50k in both training and test set\n",
    "\n",
    "train_data.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624f278",
   "metadata": {},
   "source": [
    "### Build Logistic Regressor\n",
    "Last but not least, you can build the classifier. Pyspark has an API called LogisticRegression to perform logistic regression.\n",
    "\n",
    "You initialize lr by indicating the label column and feature columns. You set a maximum of 10 iterations and add a regularization parameter with a value of 0.3. Note that in the next section, you will use cross-validation with a parameter grid to tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "676c72c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21ed9ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.022261695001277962,-0.05032306829157042,0.010203541364465971,-0.14690249353039908,-0.028731828187010432,0.3079565205336943,0.24483571267941676,-0.45163240060880216,-0.13944916605346339,-0.04041437957688153,0.23396170840124259,0.37833938300269637,0.02429212251952236,-0.2237891723229518,-0.017442166499909727,-0.20776352656917138,-0.2635618813554921,0.5080414527391149,-0.25121934098108106,-0.16855515865642703,0.5318433794982383,-0.22298649788601912,-0.22314315429737694,0.4029854393132571,-0.3118249867981298,-0.1717767367800661,-0.18707730975554712,-0.14760345320392115,-0.1266490200267906,0.23604783168497237,-0.03695994909427555,0.34301177276749417,-0.09145629959558013,0.05635663170655678,-0.2544672496517156,-0.15888236142218015,-0.14757909351234558,-0.0703133372682584,-0.23995758879069964,-0.2873179640664358,0.15073914750751394,0.16606724019136448,-0.23281713292315143,0.3272820732356272,-0.18226702580987747,-0.28786648805884874,-0.2161959064074811,0.4789179777821609,0.09498307744059162,-0.09903536727590631,0.014882963129326361,-0.16426505380782924,0.18714616509596316,0.0733481606025126,-0.22358091058868115,-0.03866292508283987,0.11535616813948574,0.14988754924808717,0.0957243983537863,-0.13828712164057574,-0.12887900065310584,0.07821601104729368,0.0356889201833419,0.05656772378597017,-0.09082657645206507,-0.19765364268577137,-0.11310459277662653,0.15844821633568293,-0.24222659475025282,-0.2079888139628788,-0.18002148029205753,0.23273099330200436,-0.031683767868536765,-0.32178630002708714,0.09314110179452784,-0.0693134895291356,0.10272031831973079,-0.10976921919514537,-0.21670209833529958,-0.20771444846642034,0.2517279842046847,-0.20966767534111402,-0.19882879981892906,-0.09262247781935076,0.07993040423903537,0.3167605454884796,-0.11027439287479984,-0.11852298032512898,-0.11971753098630769,0.16518726120507127,-0.3496257594048069,-0.1669885027361446,0.03800845526529489,0.00486371586309248,0.008387528613889321,9.076871257557419e-08,1.8934966515675005e-05,0.0629000359084324,0.00022256293235181667,0.009446427377288546]\n",
      "Intercept: -3.1076887509361706\n"
     ]
    }
   ],
   "source": [
    "# You can see the coefficients from the regression\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434c17c",
   "metadata": {},
   "source": [
    "## Step 5: Train and evaluate the model\n",
    "To generate predictions for your test set,\n",
    "\n",
    "You can use linearModel with transform() on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "790b14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can print the variables in predictions\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2140c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in the label, prediction and the probability\n",
    "\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e2a32",
   "metadata": {},
   "source": [
    "## Evalutate the model\n",
    "You need to look at the accuracy metric to see how well (or bad) the model performs. Currently, there is no API to compute the accuracy measure in Spark. The default value is the ROC, receiver operating characteristic curve. It is a different metric that take into account the false positive rate.\n",
    "\n",
    "Before you look at the ROC, let's construct the accuracy measure. You are more familiar with this metric. The accuracy measure is the sum of the correct prediction over the total number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You create a DataFrame with the label and the prediction.\n",
    "cm = predictions.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check the number of class in the label and the prediction\n",
    "cm.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a0d1d",
   "metadata": {},
   "source": [
    "For instance, in the test set, there is 1568 household with an income above 50k and 4934 below. The classifier, however, predicted 651 households with income above 50k.\n",
    "\n",
    "Your numbers can be slightly different!!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b580b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can compute the accuracy by computing the count when the label \n",
    "# is correctly classified over the total number of rows.\n",
    "cm.filter(cm.label == cm.prediction).count() / cm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefc460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can wrap everything together and write a function to compute the accuracy.\n",
    "\n",
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d820c20",
   "metadata": {},
   "source": [
    "#### ROC metrics\n",
    "The module BinaryClassificationEvaluator includes the ROC measures. The Receiver Operating Characteristic curve is another common tool used with binary classification. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve shows the true positive rate (i.e. recall) against the false positive rate. The false positive rate is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate. The true negative rate is also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1 - specificity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5c229",
   "metadata": {},
   "source": [
    "## Step 6 (Stretch): Tune the hyperparameter\n",
    "Last but not least, you can tune the hyperparameters. Similar to scikit-learn you create a parameter grid, and you add the parameters you want to tune.\n",
    "\n",
    "To reduce the time of the computation, you only tune the regularization parameter with only two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d9c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, you evaluate the model with using the cross-validation method with 5 folds. It takes some time to train.\n",
    "\n",
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best regularization hyperparameter is 0.01, with an accuracy of 85.316 percent.\n",
    "\n",
    "accuracy_m(model = cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can extract the recommended parameter by chaining cvModel.bestModel with extractParamMap()\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee3b96",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "To begin with Spark, you need to initiate a Spark Context with:\n",
    "\n",
    "SparkContext()\n",
    "\n",
    "and SQL Context to connect to different data sources:\n",
    "\n",
    "SQLContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fb8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
