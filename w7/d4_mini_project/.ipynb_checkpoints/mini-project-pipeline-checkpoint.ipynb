{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan predictions\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We want to automate the loan eligibility process based on customer details that are provided as online application forms are being filled. You can find the dataset [here](https://drive.google.com/file/d/1h_jl9xqqqHflI5PsuiQd_soNYxzFfjKw/view?usp=sharing). These details concern the customer's Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and other things as well. \n",
    "\n",
    "|Variable| Description|\n",
    "|: ------------- |:-------------|\n",
    "|Loan_ID| Unique Loan ID|\n",
    "|Gender| Male/ Female|\n",
    "|Married| Applicant married (Y/N)|\n",
    "|Dependents| Number of dependents|\n",
    "|Education| Applicant Education (Graduate/ Under Graduate)|\n",
    "|Self_Employed| Self employed (Y/N)|\n",
    "|ApplicantIncome| Applicant income|\n",
    "|CoapplicantIncome| Coapplicant income|\n",
    "|LoanAmount| Loan amount in thousands|\n",
    "|Loan_Amount_Term| Term of loan in months|\n",
    "|Credit_History| credit history meets guidelines|\n",
    "|Property_Area| Urban/ Semi Urban/ Rural|\n",
    "|Loan_Status| Loan approved (Y/N)\n",
    "\n",
    "\n",
    "\n",
    "### Explore the problem in following stages:\n",
    "\n",
    "1. Hypothesis Generation – understanding the problem better by brainstorming possible factors that can impact the outcome\n",
    "2. Data Exploration – looking at categorical and continuous feature summaries and making inferences about the data.\n",
    "3. Data Cleaning – imputing missing values in the data and checking for outliers\n",
    "4. Feature Engineering – modifying existing variables and creating new ones for analysis\n",
    "5. Model Building – making predictive models on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis Generation\n",
    "\n",
    "Generating a hypothesis is a major step in the process of analyzing data. This involves understanding the problem and formulating a meaningful hypothesis about what could potentially have a good impact on the outcome. This is done BEFORE looking at the data, and we end up creating a laundry list of the different analyses which we can potentially perform if data is available.\n",
    "\n",
    "#### Possible hypotheses\n",
    "Which applicants are more likely to get a loan\n",
    "\n",
    "1. Applicants having a credit history \n",
    "2. Applicants with higher applicant and co-applicant incomes\n",
    "3. Applicants with higher education level\n",
    "4. Properties in urban areas with high growth perspectives\n",
    "\n",
    "Do more brainstorming and create some hypotheses of your own. Remember that the data might not be sufficient to test all of these, but forming these enables a better understanding of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE HYPOTHESIS NOTEBOOK IN THIS DIRECTORY for work through on some of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. credit rating - or current credit debt\n",
    "6. coapplicants history and credit rating\n",
    "7. previous default on loan or previous bankruptcy\n",
    "8. not just history but how the repayment history is\n",
    "9. amount asked relative to income or total income if married their income as well\n",
    "10. payment period vs amount (monthly payments relative to income)\n",
    "11. any assest or loans already held"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "Let's do some basic data exploration here and come up with some inferences about the data. Go ahead and try to figure out some irregularities and address them in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"data/data.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key challenges in any data set are missing values. Lets start by checking which columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Missing values:\n",
    "# Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term, Credit_History\n",
    "# Look at each variable below individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some basic statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_feats = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ApplicantIncome'\n",
    "# Seems to be skewed by some high incomes looking and median and mean\n",
    "print(f'median income: {df.ApplicantIncome.median()}')\n",
    "print(f'mean income: {df.ApplicantIncome.mean()}')      \n",
    "print(f'max income: {df.ApplicantIncome.max()}')   \n",
    "print(f'min income: {df.ApplicantIncome.min()}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoapplicantIncome\n",
    "# This makes sense as a number of applications don't have coapplicants \n",
    "print(f'median Coapplicant income: {df.CoapplicantIncome.median()}')\n",
    "print(f'mean Coapplicant income: {df.CoapplicantIncome.mean()}')      \n",
    "print(f'max Coapplicant income: {df.CoapplicantIncome.max()}')   \n",
    "print(f'min Coapplicant income: {df.CoapplicantIncome.min()}')  \n",
    "\n",
    "# zero's DO make sense here\n",
    "# df[df.CoapplicantIncome ==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoanAmount - HAS MISSING VALUES\n",
    "print(f'missing: {df.LoanAmount.isna().sum()}')\n",
    "# df.LoanAmount.isna().Index.tolist()\n",
    "print(f'percentage missing: {df.LoanAmount.isna().sum()/df.LoanAmount.count()}')\n",
    "print(f'median loan amount: {df.LoanAmount.median()}')\n",
    "print(f'mean loan amount: {df.LoanAmount.mean()}')      \n",
    "print(f'max loan amount: {df.LoanAmount.max()}')   \n",
    "print(f'min loan amount: {df.LoanAmount.min()}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Amount_Term, - HAS MISSING VALUES\n",
    "print(f'missing: {df.Loan_Amount_Term.isna().sum()}')\n",
    "df.Loan_Amount_Term.isna().sum()/df.Loan_Amount_Term.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit_History (categorical) - HAS MISSING VALUES (assign to 0)\n",
    "df_with_hist = df[df.Credit_History ==1].count()\n",
    "df_no_hist = df[df.Credit_History ==0].count()\n",
    "print(f'with history: {df_with_hist[\"Credit_History\"]}')\n",
    "print(f'na history: {df.Credit_History.isna().sum()}')\n",
    "print(f'no history: {df_no_hist[\"Credit_History\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term, Credit_History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df.loc[df[\"Self_Employed\"].isna() == True] # Set to mode (0)\n",
    "# df.loc[df[\"LoanAmount\"].isna() == True] # try avg and med??\n",
    "# df.loc[df[\"Loan_Amount_Term\"].isna() == True] # try avg and med??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many applicants have a `Credit_History`? (`Credit_History` has value 1 for those who have a credit history and 0 otherwise)\n",
    "2. Is the `ApplicantIncome` distribution in line with your expectation? Similarly, what about `CoapplicantIncome`?\n",
    "3. Tip: Can you see a possible skewness in the data by comparing the mean to the median, i.e. the 50% figure of a feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss nominal (categorical) variable. Look at the number of unique values in each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cat_feats = ['Gender', 'Married', 'Dependents', 'Education',\n",
    "       'Self_Employed', 'Credit_History', 'Property_Area'] #'Loan_Amount_Term'?\n",
    "\n",
    "y = ['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Gender\"].isna() == True] # wipe if credit_history and gender both Nan, wipe, otherwise set to median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Married"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore further using the frequency of different categories in each nominal variable. Exclude the ID obvious reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values\n",
    "df.loc[df[\"Married\"].isna() == True]\n",
    "\n",
    "# DROP 435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender\n",
    "df['Gender'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Married\n",
    "df['Married'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values\n",
    "df.loc[df[\"Dependents\"].isna() == True] # set to median (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Education' - no null values\n",
    "df['Education'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Values\n",
    "df.loc[df[\"Credit_History\"].isna() == True].head() #???? #DROP IF SELF_EMPLOYED AND CREDIT HISTORY NAN, otherwise fill with mode or median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Credit_History'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null Values\n",
    "# print(f'median {df.Self_Employed.median()}') (after converted)\n",
    "df.loc[df[\"Self_Employed\"].isna() == True].count() # Set to most frequent (1)\n",
    "\n",
    "# IF credit_history == NaN as well, drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Self_Employed'\n",
    "df['Self_Employed'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Credit_History'\n",
    "df['Credit_History'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Property_Area'\n",
    "df['Property_Area'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Dependents' FOR NA - ASSIGN MODE\n",
    "df['Dependents'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Loan_Amount_Term.value_counts().plot(kind='bar');\n",
    "print(f'median {df.Loan_Amount_Term.median()}')\n",
    "df.loc[df[\"Loan_Amount_Term\"].isna() == True].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution analysis\n",
    "\n",
    "Study distribution of various variables. Plot the histogram of ApplicantIncome, try different number of bins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ApplicantIncome\n",
    "x = df.ApplicantIncome\n",
    "plt.hist(x, bins=1000) \n",
    "plt.title('Applicant Income')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Applicant Income');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CoapplicantIncome'\n",
    "x = df.CoapplicantIncome\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('CoApplicant Income')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Coapplicant Income');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'TotalIncome' *NEW FEATURE\n",
    "x =  df.ApplicantIncome + df.CoapplicantIncome\n",
    "plt.hist(x, bins=1000) \n",
    "plt.title('Applicant and CoApplicant Combined Income')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Combined Income');\n",
    "\n",
    "# still has skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'LoanAmount'\n",
    "x =  df.LoanAmount\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Loan Amount')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Loan Amount');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Loan_Amount_Term'\n",
    "x =  df.Loan_Amount_Term\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Loan Term')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Loan Amount Term');\n",
    "\n",
    "# THIS IS CATEGORICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Amt_Term_ratio *NEW FEATURE\n",
    "x =  df.LoanAmount/df.Loan_Amount_Term\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Loan Amount and Term Ration')\n",
    "plt.ylabel('# of Applications')\n",
    "plt.xlabel('Loan Amount and Term Ration');\n",
    "\n",
    "# SEE IF THIS CORRELATES?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Look at box plots to understand the distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Box plot numeric features to see spread - **NEED TO REMOVE OUTLIERS FOR INCOME AND LOAN AMOUNT... ALSO EVAUATE LOAN TERMS\n",
    "df['LoanAmountx100'] = df['LoanAmount']*100\n",
    "df['Loan_Amount_Termx100'] = df['Loan_Amount_Term']*100\n",
    "df['Total_Income'] = df['ApplicantIncome'] +df['CoapplicantIncome']\n",
    "df['Loan_Amount_Term_Ratiox10000'] = (df.LoanAmount*10000/df.Loan_Amount_Term)\n",
    "df_plot = df[['ApplicantIncome', 'CoapplicantIncome', 'Total_Income', 'LoanAmountx100', 'Loan_Amount_Term_Ratiox10000']]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = sns.boxplot(data=df_plot, orient=\"h\", palette=\"Set2\")\n",
    "\n",
    "# ax = sns.boxplot(data=box_data)\n",
    "# sns.set_palette(palette=\"crest\", n_colors=1)\n",
    "sns.color_palette(\"crest\", as_cmap=True)\n",
    "\n",
    "ax.set(title='Loan Application Numeric Features', xlabel=\"amount\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log, then REmove outliers\n",
    "\n",
    "# PLOT!!\n",
    "\n",
    "# Box plot numeric features to see spread - **NEED TO REMOVE OUTLIERS FOR INCOME AND LOAN AMOUNT... ALSO EVAUATE LOAN TERMS\n",
    "df['LoanAmountx100'] = df['LoanAmount']*100\n",
    "df['Loan_Amount_Termx100'] = df['Loan_Amount_Term']*100\n",
    "df['Total_Income'] = df['ApplicantIncome'] +df['CoapplicantIncome']\n",
    "df['Loan_Amount_Term_Ratiox10000'] = (df.LoanAmount*10000/df.Loan_Amount_Term)\n",
    "df_plot = df[['ApplicantIncome', 'CoapplicantIncome', 'Total_Income', 'LoanAmountx100', 'Loan_Amount_Term_Ratiox10000']]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = sns.boxplot(data=df_plot, orient=\"h\", palette=\"Set2\")\n",
    "\n",
    "# ax = sns.boxplot(data=box_data)\n",
    "# sns.set_palette(palette=\"crest\", n_colors=1)\n",
    "sns.color_palette(\"crest\", as_cmap=True)\n",
    "\n",
    "ax.set(title='Loan Application Numeric Features', xlabel=\"amount\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of income segregated  by `Education`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_edu = df[['Education', 'ApplicantIncome']]\n",
    "df_edu.Education.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad = df[df.Education == 'Graduate']\n",
    "df_not_grad = df[df.Education == 'Not Graduate']\n",
    "\n",
    "x =  df_not_grad.Total_Income\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Total Income Not Graduate')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Total Income');\n",
    "\n",
    "print(f'Mean {df_not_grad.Total_Income.mean()}')\n",
    "print(f'Median {df_not_grad.Total_Income.median()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  df_grad.Total_Income\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Total Income Graduate')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Total Income');\n",
    "print(f'Mean {df_grad.Total_Income.mean()}')\n",
    "print(f'Median {df_grad.Total_Income.median()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the histogram and boxplot of LoanAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_data = df[['LoanAmount']] \n",
    "# ax = sns.boxplot(data=df, orient=\"h\", palette=\"Set2\")\n",
    "# plt.title('Pokemon Box Plot')\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = sns.boxplot(data=box_data)\n",
    "# sns.set_palette(palette=\"crest\", n_colors=1)\n",
    "# sns.color_palette(\"crest\", as_cmap=True)\n",
    "plt.ylabel('Loan amount')\n",
    "# plt.xlabel\n",
    "ax.set(title='Loan Amount Distribution', xlabel=\"type\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be some extreme values. Both `ApplicantIncome` and `LoanAmount` require some amount of data munging. `LoanAmount` has missing and well as extreme values values, while `ApplicantIncome` has a few extreme values, which demand deeper understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Categorical variable analysis\n",
    "\n",
    "Try to understand categorical variables in more details using `pandas.DataFrame.pivot_table` and some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# kids and married, correlation plot! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "This step typically involves imputing missing values and treating outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values\n",
    "\n",
    "Missing values may not always be NaNs. For instance, the `Loan_Amount_Term` might be 0, which does not make sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values for all columns. Use the values which you find most meaningful (mean, mode, median, zero.... maybe different mean values for different groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any rows with more than 1 NaN (13 rows)\n",
    "df = pd.read_csv(\"data/data.csv\")\n",
    "to_drop = df[df.isnull().sum(axis=1) >1].index.tolist()\n",
    "\n",
    "df = df.drop(to_drop)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Married\"].isna() == True]\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term, Credit_History\n",
    "# remove any row that has more than 1 NAN\n",
    "\n",
    "# Loan_Amount_Term - check for zeros\n",
    "# LoanAmount - HAS MISSING VALUES and 0s\n",
    "#  Credit_History (categorical) - HAS MISSING VALUES (assign to 0)\n",
    "# Dependent - (impute mode - 0)\n",
    "#  Married, \n",
    "# Self_Employed, (impute mode - 0)\n",
    "# Gender (impute mode OR drop as this has high impact)\n",
    "\n",
    "# CHECK NA AFTER DOING THIS **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Remove Outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try various options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transforming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependant - convert 3+ to 3\n",
    "df.Dependents.replace({'0':0, '1':1, '2':2, '3+':3}, inplace=True)\n",
    "\n",
    "df.Dependents.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme values\n",
    "Try a log transformation to get rid of the extreme values in `LoanAmount`. Plot the histogram before and after the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['LoanAmount_Log'] = np.log(df.LoanAmount)\n",
    "\n",
    "x =  df.LoanAmount\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Loan Amount')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Loan Amount');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  df.LoanAmount_Log\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Loan Amount - Log')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Loan Amount - Log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine both incomes as total income and take a log transformation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['Total_Income'] = df['ApplicantIncome'] +df['CoapplicantIncome']\n",
    "df['Total_Income_log'] = np.log(df['Total_Income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  df.Total_Income_log\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Total Income - log')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Combined Income');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmt_Term_Ratio_Log']=  np.log(df.LoanAmount/df.Loan_Amount_Term)\n",
    "x =  df.LoanAmt_Term_Ratio_Log\n",
    "plt.hist(x, bins=100) \n",
    "plt.title('Loan Amount - Log')\n",
    "plt.ylabel('# of Applicatins')\n",
    "plt.xlabel('Loan Amount - Log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = tips.corr()\n",
    "\n",
    "# plot the correlations\n",
    "sns.heatmap(df_corr)\n",
    "plt.title('Correlation plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with z-score\n",
    "df = pd.read_csv(\"data/data.csv\") \n",
    "df['LoanAmt_Term_Ratio_Log']=  np.log(df.LoanAmount/df.Loan_Amount_Term)\n",
    "df['Total_Income_log'] = np.log(df['ApplicantIncome'] +df['CoapplicantIncome'])\n",
    "df['LoanAmount_Log'] = np.log(df.LoanAmount)\n",
    "\n",
    "def remove_outliers(df):\n",
    "    cols = ['Total_Income_log']#, 'LoanAmt_Term_Ratio_Log', 'LoanAmountLog'] \n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return df\n",
    "\n",
    "# q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "# iqr = q75 - q25\n",
    "# cut_off = iqr * 1.5\n",
    "# lower, upper = q25 - cut_off, q75 + cut_off\n",
    "\n",
    "# df.Total_Income_log\n",
    "# LoanAmountLog, LoanAmt_Term_Ratio_Log, 'Total_Income_log'\n",
    "\n",
    "df_or = remove_outliers(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_or.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay # want the ones with false neg not positives\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, plot_roc_curve, roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn import set_config # for plotting pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"data/data.csv\") \n",
    "\n",
    "# # REMOVE any rows with more than one null for training \n",
    "to_drop = df[df.isnull().sum(axis=1) >1].index.tolist()\n",
    "df = df.drop(to_drop)\n",
    "\n",
    "# Note this removes 13 rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(552, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove outliers \n",
    "def remove_outliers(df, cols):\n",
    "#     cols = ['Total_Income_log']#, 'LoanAmt_Term_Ratio_Log', 'LoanAmountLog'] \n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "#     df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    df = df[~((df[cols] > (Q3 + 2 * IQR))).any(axis=1)]\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, ['ApplicantIncome', 'LoanAmount'])\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG IMPUTER \n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out target, and drop id column\n",
    "X = df.drop(columns=['Loan_Status','Loan_ID'])\n",
    "y = df['Loan_Status'].replace({'Y':1, 'N':0})\n",
    "\n",
    "# Test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into cat_feats and num_feats\n",
    "cat_feats = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
    "num_feats = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n",
    "\n",
    "def numFeat(data):\n",
    "    return data[num_feats]\n",
    "\n",
    "def catFeat(data):\n",
    "    return data[cat_feats]\n",
    "\n",
    "keep_num = FunctionTransformer(numFeat)\n",
    "keep_cat = FunctionTransformer(catFeat)\n",
    "\n",
    "# Note: Loan amount term is really more categorical, but leaving as numeric so can use in calculations - and will scale\n",
    "# Credit history will need to be converted to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns\n",
    "def replace_income_with_total_income_log(X):\n",
    "    X['Total_Income_Log'] = np.log(X['ApplicantIncome'] + X['CoapplicantIncome'])\n",
    "    X.drop(columns=['ApplicantIncome','CoapplicantIncome'], inplace=True)\n",
    "    return X\n",
    "\n",
    "def add_LoanAmt_Term_Ratio_Log(X):\n",
    "    X['LoanAmt_Term_Ratio_Log']=  np.log(X.LoanAmount/X.Loan_Amount_Term)\n",
    "    return X\n",
    "\n",
    "def replace_loanamount_with_loanamount_log(X):\n",
    "    X['LoanAmount_Log'] = np.log(X.LoanAmount)\n",
    "    X = X.drop(columns=['LoanAmount'])\n",
    "    return X\n",
    "\n",
    "\n",
    "add_total_income_log_object = FunctionTransformer(replace_income_with_total_income_log)\n",
    "add_loanamt_term_ratio_log_object = FunctionTransformer(add_LoanAmt_Term_Ratio_Log)\n",
    "add_loanamount_log_object = FunctionTransformer(replace_loanamount_with_loanamount_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode # only a portion of the categorical \n",
    "enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - reduce dummy variables of catigorical?? or all>\n",
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbest - right now just on numeric \n",
    "selection = SelectKBest(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try paramater grid search to improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PCA/ kbest - do together after? or separately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# test with p-value <0.05, sensitivity and specifity , AUC and ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 5. Using Pipeline\n",
    "If you didn't use pipelines before, transform your data prep, feat. engineering and modeling steps into Pipeline. It will be helpful for deployment.\n",
    "\n",
    "The goal here is to create the pipeline that will take one row of our dataset and predict the probability of being granted a loan.\n",
    "\n",
    "`pipeline.predict(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ('impute_mean', SimpleImputer(strategy='median'))\n",
    "# df = pd.read_csv(\"data/data.csv\") \n",
    "# df\n",
    "# df.LoanAmount[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "# imputer = imputer.fit(X_train[num_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf = imputer.transform(X_train[num_feats])\n",
    "# np.isnan(newdf).sum()\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_pipeline = make_pipeline(   \n",
    "#     FeatureUnion(transformer_list=[\n",
    "#         ('Handle numeric columns', make_pipeline(\n",
    "#             ColumnSelector(columns=['Amount']),\n",
    "#             SimpleImputer(strategy='constant', fill_value=0),\n",
    "#             StandardScaler()\n",
    "#         )),\n",
    "#         ('Handle categorical data', make_pipeline(\n",
    "#             ColumnSelector(columns=['Type', 'Name', 'Changes']),\n",
    "#             SimpleImputer(strategy='constant', missing_values=' ', fill_value='missing_value'),\n",
    "#             OneHotEncoder(sparse=False)\n",
    "#         ))\n",
    "#     ])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pipeline = Pipeline([\n",
    "#     (“num_feats”, keep_num),\n",
    "#     (“impute_num”, null_replace_num),\n",
    "#     (“kBest”, k_best)\n",
    "# ])\n",
    "# cat_pipeline = Pipeline([\n",
    "#     (“cat_feats”, keep_cat),\n",
    "#     (“impute_cat”, null_replace_cat),\n",
    "#     (“dummies”, ohe),\n",
    "#     (“to_dense”, to_dense),\n",
    "#     (“pca”, pca)\n",
    "# ])\n",
    "# all_features = FeatureUnion([\n",
    "#     (‘numeric_features’, num_pipeline),\n",
    "#     (‘categorical_features’, cat_pipeline),\n",
    "# ])\n",
    "# main_pipeline = Pipeline([\n",
    "#     (‘all_features’, all_features),\n",
    "#     (‘modeling’, base_model)\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_num = FunctionTransformer(num_feats)\n",
    "# keep_cat = FunctionTransformer(cat_feats)\n",
    "\n",
    "numeric_pipeline = Pipeline([('num_feats', keep_num),\n",
    "#                             ('impute_median', SimpleImputer(strategy='median')),\n",
    "                            ('add_total_income', add_total_income_log_object),\n",
    "                            ('add_loanamt_term_ratio_log', add_loanamt_term_ratio_log_object),\n",
    "                            ('add_loanamount_log', add_loanamount_log_object),\n",
    "                            ('scaling', StandardScaler()),\n",
    "                            (\"kbest\", selection)]) \n",
    "\n",
    "\n",
    "categorical_pipeline = Pipeline([('cat_feats', keep_cat),\n",
    "                                ('impute_mode', SimpleImputer(strategy='most_frequent')), \n",
    "                                  ('one-hot-encode', OneHotEncoder(sparse=False)),\n",
    "                                 (\"pca\", pca)])\n",
    "\n",
    "all_features = FeatureUnion([('numeric_features', numeric_pipeline),\n",
    "                            ('categorical_features', categorical_pipeline)])\n",
    "\n",
    "# preprocessing_loan_feats = ColumnTransformer([('numeric', numeric_transform, num_feats), \n",
    "#                                         ('categorical', categorical_transform, cat_feats)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [('all_features', all_features),\n",
    "                     (\"model\", LogisticRegression())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_test = X_test.dropna()\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision =  precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(f'Test set accuracy: {acc}')\n",
    "print(f'Test set recall: {recall}')\n",
    "print(f'Precision: {precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                   estimator_name='example estimator')\n",
    "display.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm) \n",
    "disp.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to visualize pipeline\n",
    "# set_config(display='diagram')\n",
    "# pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline = Pipeline([('num_feats', keep_num),\n",
    "#                             ('impute_median', SimpleImputer(strategy='median')),\n",
    "                            ('add_total_income', add_total_income_log_object),\n",
    "                            ('add_loanamt_term_ratio_log', add_loanamt_term_ratio_log_object),\n",
    "                            ('add_loanamount_log', add_loanamount_log_object),\n",
    "                            ('scaling', StandardScaler()),\n",
    "                            (\"kbest\", selection)]) \n",
    "\n",
    "\n",
    "categorical_pipeline = Pipeline([('cat_feats', keep_cat),\n",
    "                                ('impute_mode', SimpleImputer(strategy='most_frequent')), \n",
    "                                  ('one-hot-encode', OneHotEncoder(sparse=False)),\n",
    "                                 (\"pca\", pca)])\n",
    "\n",
    "all_features = FeatureUnion([('numeric_pipeline', numeric_pipeline),\n",
    "                            ('categorical_pipeline', categorical_pipeline)])\n",
    "\n",
    "pipeline = Pipeline(steps = [('all_features', all_features),\n",
    "                     (\"model\", LogisticRegression())])\n",
    "\n",
    "param_grid = {'all_features__categorical_pipeline__pca__n_components':[3,5,7],\n",
    "              'all_features__numeric_pipeline__kbest__k': [1,2,3,4]}\n",
    "              \n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best_model = grid_ridge.best_estimator_\n",
    "# best_hyperparams = grid_ridge.best_params_\n",
    "# best_score = grid_ridge.best_score_\n",
    "\n",
    "print(f'hyperparameters: {best_hyperparams}\\n {best_score}')\n",
    "# y_pred = grid_ridge.predict(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# pipeline = Pipeline(steps = [('all_features', all_features),\n",
    "#                      (\"model\", LogisticRegression())])\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# X_test = X_test.dropna()\n",
    "\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred, average='macro')\n",
    "# precision =  precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "# print(f'Test set accuracy: {acc}')\n",
    "# print(f'Test set recall: {recall}')\n",
    "# print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, save the HTML to a file\n",
    "from sklearn.utils import estimator_html_repr\n",
    "\n",
    "with open('images/model_pipeline.html', 'w') as f:  \n",
    "    f.write(estimator_html_repr(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [('all_features', all_features),\n",
    "                     (\"model\", LogisticRegression())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_test = X_test.dropna()\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision =  precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(f'Test set accuracy: {acc}')\n",
    "print(f'Test set recall: {recall}')\n",
    "print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {#'preprocessing_loan_feats__categorical__pca__n_components':[3,5,7],\n",
    "              #'preprocessing_loan_feats__numeric__kbest__k': [1,2,3,4],\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "              'model__max_depth': [3, 7, 10, 20]\n",
    "             }\n",
    "\n",
    "pipeline = Pipeline(steps = [('all_features', all_features),\n",
    "                     (\"model\", RandomForestClassifier())])\n",
    "              \n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "best_hyperparams = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "\n",
    "print(f'hyperparameters: {best_hyperparams}\\n {best_score}')\n",
    "# y_pred = grid_ridge.predict(df_test)\n",
    "\n",
    "\n",
    "# X_test = X_test.dropna()\n",
    "\n",
    "# y_pred = grid.predict(X_test)\n",
    "\n",
    "\n",
    "# # pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# recall = recall_score(y_test, y_pred, average='macro')\n",
    "# precision =  precision_score(y_test, y_pred, average='micro') \n",
    "\n",
    "# print(f'Test set accuracy: {acc}')\n",
    "# print(f'Test set recall: {recall}')\n",
    "# print(f'Precision: {precision}')\n",
    "\n",
    "# plot_roc_curve(pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "param_grid_ridge = {'preprocessing_sales__categorical__pca__n_components':[3,5,7],\n",
    "              'preprocessing_sales__numeric__kbest__k': [1,2,3,4]}\n",
    "              \n",
    "\n",
    "grid_ridge = GridSearchCV(pipeline_ridge, param_grid=param_grid_ridge, cv=5)\n",
    "grid_ridge.fit(df_train, y_train)\n",
    "\n",
    "best_model = grid_ridge.best_estimator_\n",
    "best_hyperparams = grid_ridge.best_params_\n",
    "best_score = grid_ridge.best_score_\n",
    "\n",
    "print(f'hyperparameters: {best_hyperparams}\\n {best_score}')\n",
    "# y_pred = grid_ridge.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm) \n",
    "disp.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [('all_features', all_features),\n",
    "                     (\"model\",  GaussianNB())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "X_test = X_test.dropna()\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "precision =  precision_score(y_test, y_pred, average='micro') \n",
    "\n",
    "print(f'Test set accuracy: {acc}')\n",
    "print(f'Test set recall: {recall}')\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "# plot_roc_curve(pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm) #,display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = lr.coef_[0]\n",
    "\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test_b, clf.predict(X_test_b))\n",
    "fpr, tpr, thresholds = roc_curve(y_test_b, clf.predict_proba(X_test_b)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy your model to cloud and test it with PostMan, BASH or Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one and many samples\n",
    "# Docker \n",
    "# curl \n",
    "# pickle file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
