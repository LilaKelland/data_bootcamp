{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Pipelining in Sklearn \n",
    "\n",
    "What is Pipelining? \n",
    "- Pipelining chains multiple steps together. The output of step 1 will be the input to step 2. Essentially is a list of ordered instruction we want to use to preprocess our train and test data. \n",
    "- Makes life easy.\n",
    "\n",
    "\n",
    "Tips modified from: https://github.com/justmarkham/scikit-learn-tips\n",
    "\n",
    "^recommended repo for further sklearn tips"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.DataFrame({'feat1':[10, 20, np.nan, 2], 'feat2':[25., 20, 5, 3], 'label':['A', 'A', 'B', 'B']})\n",
    "test = pd.DataFrame({'feat1':[30., 5, 15], 'feat2':[12, 10, np.nan]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "imputer = SimpleImputer()\n",
    "log_reg = LogisticRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Using Make pipeline, making a two step pipeline\n",
    "pipe = make_pipeline(imputer,\n",
    "                    log_reg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = ['feat1', 'feat2']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X, y = train[features], train['label']\n",
    "X_new = test[features]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pipeline applies the imputer to X before fitting the classifier\n",
    "pipe.fit(X, y)\n",
    "\n",
    "# pipeline applies the imputer to X_new before making predictions\n",
    "# note: pipeline uses imputation values learned during the \"fit\" step\n",
    "pipe.predict(X_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline vs. make_pipline in Sklearn\n",
    "\n",
    "Pipeline requires naming of steps, make_pipeline does not. **It is easier to use make_pipeline, less syntax.**\n",
    "\n",
    "- I would recommend use make_pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('http://bit.ly/kaggletrain', nrows=6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cols = ['Embarked', 'Sex', 'Age', 'Fare']\n",
    "X = df[cols]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "imp = SimpleImputer()\n",
    "clf = LogisticRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Pass tuples, (object, list of columns to apply object)\n",
    "ct = make_column_transformer(\n",
    "    (ohe, ['Embarked', 'Sex']),\n",
    "    (imp, ['Age']),\n",
    "    remainder='passthrough')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipe = make_pipeline(ct, clf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#uses more code, you pass a list of tuples, each tuple requries (made up name, object, list of columns)\n",
    "ct = ColumnTransformer(\n",
    "    [('encoder', ohe, ['Embarked', 'Sex']),\n",
    "     ('imputer', imp, ['Age'])],\n",
    "    remainder='passthrough') #any columns not named, pass through"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipe = Pipeline([('preprocessor', ct), ('classifier', clf)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Function Transformer to build customer transformers. \n",
    "\n",
    "Use Case: Feature engineering within a Column Transformer or Pipeline.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = pd.DataFrame({'Fare':[200, 300, 50, 900],\n",
    "                  'Code':['X12', 'Y20', 'Z7', np.nan],\n",
    "                  'Deck':['A101', 'C102', 'A200', 'C300']})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert existing fucntion into a transformer\n",
    "clip_values = FunctionTransformer(np.clip, kw_args={'a_min':100, 'a_max':600}) #provides a lower and upper limit to value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert custom function into a transformer\n",
    "# extract the first letter from each string\n",
    "def first_letter(df):\n",
    "    return df.apply(lambda x: x.str.slice(0, 1))\n",
    "\n",
    "#apply function to Function Transformer\n",
    "get_first_letter = FunctionTransformer(first_letter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ct = make_column_transformer(\n",
    "    (clip_values, ['Fare']),\n",
    "    (get_first_letter, ['Code', 'Deck']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ct.fit_transform(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Pipeline Examples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cols = ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('http://bit.ly/kaggletrain')\n",
    "X = df[cols]\n",
    "y = df['Survived']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_new = pd.read_csv('http://bit.ly/kaggletest', nrows=10)\n",
    "X_new = df_new[cols]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector, make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, make_union, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Below is 4 different methods using various combinations of the above functions, they all result in the same thing. Besides method 4. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Method 1 with make_column_selector and make pipeline : BEST Method\n",
    "\n",
    "# set up preprocessing for numeric columns\n",
    "imp_median = SimpleImputer(strategy='median', add_indicator=True)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# set up preprocessing for categorical columns\n",
    "imp_constant = SimpleImputer(strategy='constant')\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# select columns by data type\n",
    "num_cols = make_column_selector(dtype_include='number')\n",
    "cat_cols = make_column_selector(dtype_exclude='number')\n",
    "\n",
    "\n",
    "# do all preprocessing\n",
    "preprocessor = make_column_transformer(\n",
    "    (make_pipeline(imp_median, scaler), num_cols),\n",
    "    (make_pipeline(imp_constant, ohe), cat_cols))\n",
    "\n",
    "# create a pipeline\n",
    "pipe = make_pipeline(preprocessor, LogisticRegression())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Method 2 - More complicated with more steps \n",
    "cat_feats = X.dtypes[X.dtypes == 'object'].index.tolist()\n",
    "num_feats = X.dtypes[~X.dtypes.index.isin(cat_feats)].index.tolist()\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Using own function in Pipeline\n",
    "def numFeat(data):\n",
    "    return data[num_feats]\n",
    "\n",
    "def catFeat(data):\n",
    "    return data[cat_feats]\n",
    "\n",
    "# we will start two separate pipelines for each type of features\n",
    "keep_num = FunctionTransformer(numFeat)\n",
    "keep_cat = FunctionTransformer(catFeat)\n",
    "\n",
    "pipe_num = Pipeline([\n",
    "    (\"num_feats\", keep_num),\n",
    "    (\"inpute_num\", imp_median),\n",
    "    (\"scaler\", scaler)\n",
    "])\n",
    "\n",
    "pipe_cat = Pipeline([\n",
    "    ('cat_feats', keep_cat),\n",
    "    ('inpute_cat', imp_constant),\n",
    "    ('ohe', ohe)\n",
    "])\n",
    "\n",
    "union = FeatureUnion([('num_process', pipe_num), #Feature Union runs things in parrelel \n",
    "                     ('cat_process', pipe_cat)])\n",
    "\n",
    "##OR use make_union, it is a short hand version of Feature Union\n",
    "#make_union = make_union(pipe_num,pipe_cat)\n",
    "\n",
    "pipe = Pipeline([('all_features', union), \n",
    "                ('model', LogisticRegression())])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Method 3 - Similar to Method 2 but with columntransformer, and doesn't require Keep_num and Keep_cat, uses the make_column_selector\n",
    "\n",
    "# select columns by data type\n",
    "num_cols = make_column_selector(dtype_include='number')\n",
    "cat_cols = make_column_selector(dtype_exclude='number')\n",
    "\n",
    "pipe_num = Pipeline([\n",
    "    (\"inpute_num\", imp_median),\n",
    "    (\"scaler\", scaler)\n",
    "])\n",
    "\n",
    "pipe_cat = Pipeline([\n",
    "    ('inpute_cat', imp_constant),\n",
    "    ('ohe', ohe)\n",
    "])\n",
    "\n",
    "pre_process = ColumnTransformer([(\"num\", pipe_num, num_cols),\n",
    "                                (\"cat\",pipe_cat, cat_cols)],\n",
    "                               remainder = 'passthrough') #Any columns not selected in the above two steps will not be modified\n",
    "    \n",
    "pipe = Pipeline([('all_features', pre_process), \n",
    "                ('model', LogisticRegression())])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Method 4 - Similar to Method 1 but with Function Transformer, where we create a custom transformation. \\\\\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#multiply fare by 10 and have a new column\n",
    "def fare_x10(df):\n",
    "    return df.apply(lambda x: x * 10)\n",
    "\n",
    "#Apply function to FunctionTransfomer, needed to fit into a columntransformer or make_column transformer\n",
    "fare_x10_function = FunctionTransformer(fare_x10)\n",
    "\n",
    "# set up preprocessing for numeric columns\n",
    "imp_median = SimpleImputer(strategy='median', add_indicator=True)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# set up preprocessing for categorical columns\n",
    "imp_constant = SimpleImputer(strategy='constant')\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# select columns by data type\n",
    "num_cols = make_column_selector(dtype_include='number')\n",
    "cat_cols = make_column_selector(dtype_exclude='number')\n",
    "\n",
    "\n",
    "# do all preprocessing\n",
    "preprocessor = make_column_transformer(\n",
    "    (fare_x10_function, [\"Fare\"]),\n",
    "    (make_pipeline(imp_median, scaler), num_cols),\n",
    "    (make_pipeline(imp_constant, ohe), cat_cols), remainder = 'passthrough')\n",
    "\n",
    "# create a pipeline\n",
    "pipe = make_pipeline(preprocessor, RandomForestClassifier())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cross-validate the pipeline\n",
    "cross_val_score(pipe, X, y).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fit the pipeline and make predictions\n",
    "pipe.fit(X, y)\n",
    "pipe.predict(X_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import joblib\n",
    "joblib.dump(pipe, \"pipe.joblib\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#load pipeline\n",
    "model = joblib.load('pipe.joblib')\n",
    "\n",
    "model.predict(X_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('the_one': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "0945760d4fccd0fad09b80ecf3e876c141f671a1f38ac8c10c9cd6c11c7c4e94"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}