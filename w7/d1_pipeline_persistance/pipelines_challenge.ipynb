{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, we will be working with this [dataset](https://drive.google.com/file/d/1B07fvYosBNdIwlZxSmxDfeAf9KaygX89/view?usp=sharing), where we will be predicting sales. \n",
    "\n",
    "**The main goal is to create a `pipeline` that covers all the data preprocessing and modeling steps.**\n",
    "\n",
    "\n",
    "**TASK 1**: Build a pipeline that ends with a regression model, to predict `Item_Outlet_Sales` from the dataset. \n",
    "\n",
    "**The pipeline should have following steps:**\n",
    "\n",
    "1. Split the features into numerical and categorical (text)\n",
    "2. Replace null values\n",
    "    - the mean for numerical variables\n",
    "    - the most frequent value for categorical variables\n",
    "3. Create dummy variables from categorical features\n",
    "4. Use a PCA to reduce number of dummy variables to 3 principal components. PCA will be used directly after the OneHotEncoder that outputs data into a SparseMatrix, so we will need to use the **ToDenseTransformer** from the [article about custom pipelines](https://queirozf.com/entries/scikit-learn-pipelines-custom-pipelines-and-pandas-integration).\n",
    "5. Select the 3 best candidates from the original numerical features using KBest\n",
    "6. Fit a Ridge regression (default alpha is fine for now)\n",
    "\n",
    "**TASK 2**: Tune the parameters of multiple models as well as the preprocessing steps and find the best solution.\n",
    "- Try these models: \n",
    "        - Random Forest Regressor\n",
    "        - Gradient Boosting Regressor \n",
    "        - Ridge Regression. \n",
    "- For the task 2, we will need to use the same approach from this [earlier article](https://iaml.it/blog/optimizing-sklearn-pipelines), in the section `PIPELINE TUNING (ADVANCED VERSION)`, where we tried different kinds of scalers. (Use the article as reference.)\n",
    "\n",
    "_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
       "0           FDA15         9.30          Low Fat         0.016047   \n",
       "1           DRC01         5.92          Regular         0.019278   \n",
       "2           FDN15        17.50          Low Fat         0.016760   \n",
       "3           FDX07        19.20          Regular         0.000000   \n",
       "4           NCD19         8.93          Low Fat         0.000000   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                  Dairy  249.8092            OUT049   \n",
       "1            Soft Drinks   48.2692            OUT018   \n",
       "2                   Meat  141.6180            OUT049   \n",
       "3  Fruits and Vegetables  182.0950            OUT010   \n",
       "4              Household   53.8614            OUT013   \n",
       "\n",
       "   Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "0                       1999      Medium               Tier 1   \n",
       "1                       2009      Medium               Tier 3   \n",
       "2                       1999      Medium               Tier 1   \n",
       "3                       1998         NaN               Tier 3   \n",
       "4                       1987        High               Tier 3   \n",
       "\n",
       "         Outlet_Type  Item_Outlet_Sales  \n",
       "0  Supermarket Type1          3735.1380  \n",
       "1  Supermarket Type2           443.4228  \n",
       "2  Supermarket Type1          2097.2700  \n",
       "3      Grocery Store           732.3800  \n",
       "4  Supermarket Type1           994.7052  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/regression_exercise.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Low Fat', 'Regular', 'low fat', 'LF', 'reg'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Item_Fat_Content.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating target variable\n",
    "y = df[\"Item_Outlet_Sales\"]\n",
    "df = df.drop([\"Item_Outlet_Sales\",\"Item_Identifier\"],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into a train and test set.\n",
    "\n",
    "**Note:** We should always do this at the beginning before the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test train THEN \n",
    "df_train = df.sample(frac=0.8).sort_index()\n",
    "y_train = y[y.index.isin(df_train.index.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[~df.index.isin(df_train.index.tolist())].sort_index()\n",
    "y_test = y[y.index.isin(df_test.index.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Weight                  float64\n",
       "Item_Fat_Content              object\n",
       "Item_Visibility              float64\n",
       "Item_Type                     object\n",
       "Item_MRP                     float64\n",
       "Outlet_Identifier             object\n",
       "Outlet_Establishment_Year      int64\n",
       "Outlet_Size                   object\n",
       "Outlet_Location_Type          object\n",
       "Outlet_Type                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Task I\n",
    "\n",
    "### Split Features into numerical and categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = df.dtypes[df.dtypes == 'object'].index.tolist()\n",
    "num_feats = df.dtypes[~df.dtypes.index.isin(cat_feats)].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item_Fat_Content',\n",
       " 'Item_Type',\n",
       " 'Outlet_Identifier',\n",
       " 'Outlet_Size',\n",
       " 'Outlet_Location_Type',\n",
       " 'Outlet_Type']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Using own function in Pipeline\n",
    "def numFeat(data):\n",
    "    return data[num_feats]\n",
    "\n",
    "def catFeat(data):\n",
    "    return data[cat_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start two separate pipelines for each type of features\n",
    "keep_num = FunctionTransformer(numFeat)\n",
    "keep_cat = FunctionTransformer(catFeat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Low Fat', 'Regular'], dtype=object)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Feel like low fats should be transformed as well:\n",
    "\n",
    "def consolidate_low_fat(X):\n",
    "    X.Item_Fat_Content.replace({'low fat':'Low Fat', 'LF': 'Low Fat', 'reg':'Regular'}, inplace=True )\n",
    "    return X\n",
    "\n",
    "consolidate_low_fat_object = FunctionTransformer(consolidate_low_fat)\n",
    "\n",
    "x = consolidate_low_fat_object.fit_transform(df_train)\n",
    "x.Item_Fat_Content.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replacing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# df_num_no_null = imp.fit_transform(keep_num.fit_transform(df_train))\n",
    "\n",
    "imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# df_cat_no_null = imp.fit_transform(keep_cat.fit_transform(df_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(df_num_no_null).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "# df_cat_ohe = enc.fit_transform(df_cat_no_null) # don't fit transform - pipeline does this\n",
    "\n",
    "\n",
    "# HOW DO I GET FEATURES OUT? - you would look back with indexing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PCA to reduce the number of dummy variables to 3 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget ToDenseTransformer after one hot encoder (or set sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "# df_pca = pca.fit_transform(df_cat_ohe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the 3 best numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "selection = SelectKBest(k=3)\n",
    "# df_kbest = selection.fit_transform(df_num_no_null, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_union = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)]) # do in pipeline instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Use base_model in Task I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Ridge()\n",
    "# base_model.fit(feature_union, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transform = Pipeline([('impute_mean', SimpleImputer(strategy='mean')), \n",
    "                              ('scaling', StandardScaler()),\n",
    "                             (\"univ_select\", selection)]) # kbest\n",
    "\n",
    "\n",
    "categorical_transform = Pipeline([('fix_low_fat', consolidate_low_fat_object),\n",
    "                                  ('impute_mode', SimpleImputer(strategy='most_frequent')), \n",
    "                                  ('one-hot-encode', OneHotEncoder(sparse=False)),\n",
    "                                 (\"pca\", pca)])\n",
    "\n",
    "\n",
    "preprocessing_sales = ColumnTransformer([('numeric', numeric_transform, num_feats), \n",
    "                                        ('categorical', categorical_transform, cat_feats)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set RMSE: 1298.118054471505\n",
      "r2: 0.3918016430295822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-189.13482509,  973.04446381, -359.86393405,   39.5948291 ,\n",
       "       -629.10679271,  105.10233984])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_union = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "pipeline = Pipeline(steps = [('preprocessing_sales', preprocessing_sales),\n",
    "                     (\"regressor\", Ridge())])\n",
    "\n",
    "pipeline.fit(df_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(df_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'\\nTest set RMSE: {rmse}\\nr2: {r2}')\n",
    "pipeline.steps[1][1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## Task II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge = Ridge()\n",
    "rf = RandomForestRegressor() # n_estimators, max_depth\n",
    "gb = GradientBoostingRegressor() # learning_rate (default 01, n_estimators def 100, max_depth = 3\n",
    "\n",
    "pca = PCA() #n_components\n",
    "selection = SelectKBest() #k\n",
    "imp_num = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters: {'preprocessing_sales__categorical__pca__n_components': 7, 'preprocessing_sales__numeric__kbest__k': 2}\n",
      " 0.5599733752834932\n"
     ]
    }
   ],
   "source": [
    "numeric_transform = Pipeline([('impute_mean', SimpleImputer(strategy='mean')), \n",
    "                              ('scaling', StandardScaler()),\n",
    "                             (\"kbest\", SelectKBest())])\n",
    "\n",
    "\n",
    "categorical_transform = Pipeline([('fix_low_fat', consolidate_low_fat_object),\n",
    "                                  ('impute_mode', SimpleImputer(strategy='most_frequent')), \n",
    "                                  ('one-hot-encode', OneHotEncoder(sparse=False)),\n",
    "                                 (\"pca\", PCA())])\n",
    "\n",
    "\n",
    "preprocessing_sales = ColumnTransformer([('numeric', numeric_transform, num_feats), \n",
    "                                        ('categorical', categorical_transform, cat_feats)])\n",
    "\n",
    "pipeline_ridge = Pipeline(steps = [('preprocessing_sales', preprocessing_sales),\n",
    "                                     (\"regressor\", Ridge())])\n",
    "\n",
    "\n",
    "param_grid_ridge = {'preprocessing_sales__categorical__pca__n_components':[3,5,7],\n",
    "              'preprocessing_sales__numeric__kbest__k': [1,2,3,4]}\n",
    "              \n",
    "\n",
    "grid_ridge = GridSearchCV(pipeline_ridge, param_grid=param_grid_ridge, cv=5)\n",
    "grid_ridge.fit(df_train, y_train)\n",
    "\n",
    "best_model = grid_ridge.best_estimator_\n",
    "best_hyperparams = grid_ridge.best_params_\n",
    "best_score = grid_ridge.best_score_\n",
    "\n",
    "print(f'hyperparameters: {best_hyperparams}\\n {best_score}')\n",
    "# y_pred = grid_ridge.predict(df_test)\n",
    "\n",
    "\n",
    "# rmse = grid_ridge.mean_squared_error(y_test, y_pred, squared=False)\n",
    "# r2 = grid_ridge.r2_score(y_test, y_pred)\n",
    "# print(f'\\nTest set RMSE: {rmse}\\nr2: {r2}')\n",
    "# pipeline.steps[1][1].coef_\n",
    "# print(f'hyperparameters: {best_hyperparams}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters: {'preprocessing_sales__categorical__pca__n_components': 5, 'preprocessing_sales__numeric__kbest__k': 4, 'regressor__max_depth': 10, 'regressor__n_estimators': 200}\n",
      " 0.5799716368817872\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf = Pipeline(steps = [('preprocessing_sales', preprocessing_sales),\n",
    "                     (\"regressor\", RandomForestRegressor())])\n",
    "\n",
    "param_grid_rf = {'regressor__n_estimators': [50,100,200 ],\n",
    "              'regressor__max_depth': [10, 20, 30],\n",
    "              'preprocessing_sales__categorical__pca__n_components':[3,5,7],\n",
    "              'preprocessing_sales__numeric__kbest__k': [1,2,3,4]},\n",
    "              \n",
    "grid_rf = GridSearchCV(pipeline_rf, param_grid=param_grid_rf, cv=5, n_jobs = -1)\n",
    "grid_rf.fit(df_train, y_train)\n",
    "\n",
    "# best_model = grid_rf.best_estimator_\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "best_score = grid_rf.best_score_\n",
    "print(f'hyperparameters: {best_hyperparams}\\n {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters: {'preprocessing_sales__categorical__pca__n_components': 7, 'preprocessing_sales__numeric__kbest__k': 3, 'regressor__max_depth': 10, 'regressor__n_estimators': 50}\n",
      " 0.527652957575053\n"
     ]
    }
   ],
   "source": [
    "# learning_rate (default 01, n_estimators def 100, max_depth = 3\n",
    "pipeline_gb = Pipeline(steps = [('preprocessing_sales', preprocessing_sales),\n",
    "                     (\"regressor\", GradientBoostingRegressor())])\n",
    "\n",
    "param_grid_gb = {'regressor__n_estimators': [50,100,200 ],\n",
    "              'regressor__max_depth': [10, 20, 30],\n",
    "              'preprocessing_sales__categorical__pca__n_components':[3,5,7],\n",
    "              'preprocessing_sales__numeric__kbest__k': [1,2,3,4]},\n",
    "              \n",
    "grid_gb = GridSearchCV(pipeline_gb, param_grid=param_grid_gb, cv=5, n_jobs = -1)\n",
    "grid_gb.fit(df_train, y_train)\n",
    "\n",
    "# best_model = grid_gb.best_estimator_\n",
    "best_hyperparams = grid_gb.best_params_\n",
    "best_score = grid_gb.best_score_\n",
    "print(f'hyperparameters: {best_hyperparams}\\n {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score is:  0.6241741712069144\n"
     ]
    }
   ],
   "source": [
    "# print('Final score is: ', tuned_model.score(df_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
