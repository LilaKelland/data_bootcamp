{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cad4f8-ec78-49e0-8146-d3e93c568724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://iaml.it/blog/optimizing-sklearn-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866872e7-3edb-4c13-89ef-96be4ee24f16",
   "metadata": {},
   "source": [
    "Pipeline aka workflow\n",
    "\n",
    "Execute a sequence of typical tasks: data normalization, imputation of missing values, outlier elicitation, dimensionality reduction, classification.\n",
    "\n",
    "### Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50613f07-cbfb-4332-83c1-c702ed71cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084c9941-c240-4c90-8336-a08e988facfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import all modules for this tutorial\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df332938-e7bb-4226-af44-0aec34fc1c57",
   "metadata": {},
   "source": [
    "This pipeline is composed of the following steps:\n",
    "1. Data Normalization: in this tutorial we have selected three different normalization methods, including the QuantileTransformer.\n",
    "2. Dimensionality Reduction: we selected Principal Component Analysis (PCA) and a univariate feature selection algorithm as possible candidates.\n",
    "3. Regression: we apply a simple regularized linear method, although the method is easily extendable to other learning algorithms.\n",
    "\n",
    "#### Maunally implementing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d8fd90-17e9-4315-842e-4619599426de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dcc4190-ff17-40b5-824b-a6e48ff4e8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually pass training datase to everystep\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d45bd-bdb3-4a47-a21f-ebd6acaab3e8",
   "metadata": {},
   "source": [
    "#### Scikit-learn pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d420c6-5acd-45bf-81d0-ee0ae9ee9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reduce_dim', PCA()),\n",
    "        ('regressor', Ridge())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480245eb-3cb6-4778-b3bc-61720c312dfc",
   "metadata": {},
   "source": [
    "The pipeline module leverages on the common interface that every scikit-learn library must implement, such as: fit, transform and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d2c315-e749-4d10-b8cb-fc990e98ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score:  -12056.504018831934\n"
     ]
    }
   ],
   "source": [
    "# train and test whole pipeline\n",
    "pipe = pipe.fit(X_train, y_train)\n",
    "print('Testing score: ', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c380a7-ed8c-43d6-9d12-1d24c5e2f345",
   "metadata": {},
   "source": [
    "It is also possible to index the pipeline to access a specific element and retrieve a single value, for example the explained variance in the PCA step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91de0ceb-e404-441e-86fc-c0c63b4ad90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455\n",
      " 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455]\n"
     ]
    }
   ],
   "source": [
    "print(pipe.steps[1][1].explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4d8df-4836-4630-a344-e281187c753d",
   "metadata": {},
   "source": [
    "On every object within the pipeline the methods fit_transform are invoked during training, while transform (or predict) are called during test.\n",
    "\n",
    "#### Pipeline Tuning (base version)\n",
    "\n",
    "Let's start with example, where we aim at optimizing the number of components selected by the PCA and the regularization factor of the linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e724096-d207-4a6a-9099-2e8efa326371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concerning PCA, we want to evaluate how accuracy varies with the number of components, from 1 to 10:\n",
    "import numpy as np\n",
    "n_features_to_test = np.arange(1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "206c439e-94c8-471b-b9f2-b28d08175fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As for the regularization factor, we consider an exponential range of values (see sklearn gridsearchCV tutorial)\n",
    "alpha_to_test = 2.0**np.arange(-6, +6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39babff-6864-4e18-aa75-86c76cb40a81",
   "metadata": {},
   "source": [
    "It's possible to notice that the two parameters are correlated, and should be optimized in combination. That is, a variation in the number of PCA components might imply a variation in the regularization factor, and viceversa. Thereby, it is important to evaluate all their possible combinations, and this is where the pipeline module really supports us. First of all, we define a dictionary with all the parameters we would like to combine in the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13edce8-60ad-49c8-8aca-47b22814a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'reduce_dim__n_components': n_features_to_test,\\\n",
    "              'regressor__alpha': alpha_to_test}\n",
    "# note naming: name of pipeline step, dunder, name of parameter within step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe1c428b-41bb-4f09-ab39-5f9a1578ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "Final score is:  -2869.352576296183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gridsearch = GridSearchCV(pipe, params, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fc9a583-a6b7-4aa3-80e2-26ce0ca8c172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reduce_dim__n_components': 10, 'regressor__alpha': 32.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86265839-8fda-4eec-9ff6-965e27d8a560",
   "metadata": {},
   "source": [
    "#### Pipeline Tuning (advanced version)\n",
    "\n",
    "Use same approach to decide which algorithm we should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6de618-538f-4778-8587-c5ba66273bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  data normalization\n",
    "scalers_to_test = [StandardScaler(), RobustScaler(), QuantileTransformer()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98c27f11-0f1e-43bf-8a5c-8a85125e0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add normalization parameter to gridsearch\n",
    "params = {'scaler': scalers_to_test,\n",
    "        'reduce_dim__n_components': n_features_to_test,\\\n",
    "        'regressor__alpha': alpha_to_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f60318-8835-46e2-ac50-2a44fba86ef2",
   "metadata": {},
   "source": [
    "In theory, we could also apply the same approach to the dimensionality reduction step, for example to choose between PCA and SelectKBest. The only problem in this case is that PCA relies on a parameter named n_components, while SelectKBest requires to optimize a parameter named k.\n",
    "\n",
    "Luckily, GridSearchCV also allows to optimize lists of parameter dictionaries, which solves this issue as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac8ea3d-b18e-4c4d-9287-aade0f289fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "        {'scaler': scalers_to_test,\n",
    "         'reduce_dim': [PCA()],\n",
    "         'reduce_dim__n_components': n_features_to_test,\\\n",
    "         'regressor__alpha': alpha_to_test},\n",
    "\n",
    "        {'scaler': scalers_to_test,\n",
    "         'reduce_dim': [SelectKBest(f_regression)],\n",
    "         'reduce_dim__k': n_features_to_test,\\\n",
    "         'regressor__alpha': alpha_to_test}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53206535-33fb-4a4d-8457-50b5a90b40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch gridsearch\n",
    "gridsearch = GridSearchCV(pipe, params, verbose=1).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ef8499c-40fa-459e-b8e5-866979817c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score is:  -11771.701767234736\n"
     ]
    }
   ],
   "source": [
    "print('Final score is: ', gridsearch.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be5608bf-b524-4c68-b231-e46e9501cbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reduce_dim': SelectKBest(score_func=<function f_regression at 0x7fea1df79430>),\n",
       " 'reduce_dim__k': 10,\n",
       " 'regressor__alpha': 4.0,\n",
       " 'scaler': StandardScaler()}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f6d30-d6d7-4021-b339-ef7422723c50",
   "metadata": {},
   "source": [
    "When the overall number of hyper-parameters is very high, we might need to replace the optimization method (e.g. applying a randomized grid search)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb79e06-f018-4437-86d3-7caff675ea42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
