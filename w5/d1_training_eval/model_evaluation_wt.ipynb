{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ed9248-c9fd-4df3-9fe9-10ef5106acc8",
   "metadata": {},
   "source": [
    "## Regression Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18729b9e-c9ec-441b-8e43-02259094cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b83671-decb-4a84-9d8c-010b79d75d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will generate 10 observations (y_true) and 10 predictions (y_pred) from a model.\n",
    "\n",
    "# generate 'ground truth'\n",
    "y_true = np.random.normal(0,1,10)\n",
    "\n",
    "# generate random errors\n",
    "errors = np.random.normal(0,0.02,10)\n",
    "\n",
    "# simulate predictions\n",
    "y_pred = y_true + errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f2667-6116-4cb5-b047-ab8c5bb2af82",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE) / Root Means Squared Error (RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9705ead0-3d80-45e4-a25f-966dfa183d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00046649956601397993\n"
     ]
    }
   ],
   "source": [
    "# import MSE from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# compute MSE\n",
    "MSE = mean_squared_error(y_true,y_pred)  \n",
    "\n",
    "# print MSE\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5e9a2-81ff-4a11-a58c-d83f657c991c",
   "metadata": {},
   "source": [
    "All regression evaluation functions from sklearn.metrics take two mandatory arrays as parameters. The first is an array with ground truth values (in our case y_true variable) and the second is our prediction (in our case y_pred variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f709d2-511e-40e2-b967-4bea1af5f73e",
   "metadata": {},
   "source": [
    "#### Root Means Squared Error (RMSE)\n",
    "To get RMSE from MSE we have to options: the first option is to compute the square root from MSE by Numpy and the second option is to use the squared=False option in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d8e012-73d6-444c-bc03-44ff6351fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02159860101983413\n",
      "0.02159860101983413\n"
     ]
    }
   ],
   "source": [
    "# RMSE by Numpy\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "# RMSE by sklearn\n",
    "RMSE = mean_squared_error(y_true,y_pred,squared=False)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0079be1-80c8-460c-9ef3-50e34fb49dc4",
   "metadata": {},
   "source": [
    "## Classification Models Evaluation\n",
    "We will use the same principle as in regression model evaluation and use synthetic data. With the only difference that we will need predicted probabilities instead of predicted labels (predicted values in regression). The important thing to mention is that we are simulating the behavior of a binary classifier. It means that the predicted class is only positive (returns 1) or negative (returns 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d229523-b035-487f-b2d3-0eb46b5a7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "y_true = [1,1,0,1,0,0,1,0,0,1]\n",
    "\n",
    "# simulate probabilites of positive class\n",
    "# (probabilities of the observations from the positive class)\n",
    "y_proba = [0.9,0.7,0.2,0.99,0.7,0.1,0.5,0.2,0.4,0.6]\n",
    "\n",
    "# set the threshold to predict positive class\n",
    "thres = 0.5\n",
    "\n",
    "# class predictions\n",
    "# (All observations with probabilities above this threshold are assigned to the positive class_\n",
    "y_pred = [int(value > thres) for value in y_proba]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0853d8-c663-4760-85b5-5278015faa06",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbb1ec9-6c6c-4e57-8f94-4657b3314295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# import accuracy_score from sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_true,y_pred)\n",
    "\n",
    "# print accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0773f8-b231-46f1-bf0e-a6742b2ed769",
   "metadata": {},
   "source": [
    "### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48abb1b-e846-4d68-a506-63d764bf6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "# import f1_score from sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# compute F1-score\n",
    "f1_score = f1_score(y_true,y_pred)\n",
    "\n",
    "# print F1-score\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd9280-3b8f-4014-b707-3511b1c8cdc9",
   "metadata": {},
   "source": [
    "### AUC-score\n",
    "NOTE!! In roc_auc_score we use probabilities (y_proba) instead of class labels. If we passed class labels no errors would be shown but the score would be inaccurate. Be careful and read the documentation before using Sklearn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47106ef7-6307-472c-98aa-f1fbc385c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import roc_auc_score from sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# compute AUC-score\n",
    "auc = roc_auc_score(y_true,y_proba)\n",
    "\n",
    "# print AUC-score\n",
    "print(auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
