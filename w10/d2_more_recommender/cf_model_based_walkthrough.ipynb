{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f2ef71-90cd-4bb2-9fae-37a1257fae5a",
   "metadata": {},
   "source": [
    "https://blog.cambridgespark.com/tutorial-practical-introduction-to-recommender-systems-dbe22848392b\n",
    "\n",
    "Many traditional methods for training recommender systems are bad at making predictions due to a process known as overfitting. Overfitting in the context of recommender systems means our model will be good at fitting to the data we have, but bad at recommending new products to customers (not ideal given that is their purpose). This is because there is so much missing information\n",
    "\n",
    "#### Recommender Systems and Matrix Factorisation\n",
    "The data input for a recommender system can be thought of as a large matrix, with the rows indicating an entry for a customer, and the columns indicating an entry for a particular item. Let‚Äôs call this matrix ùëÖ. Then entry ùëÖùíæùëó will contain the score that customer ùíæ has given to product ùëó. For example if it‚Äôs a review this could be a number from 1‚Äì5, or it might just be 0‚Äì1 indicating if a user has bought an item or not. This matrix contains a lot of missing information, it‚Äôs unlikely a customer has bought every item on Amazon! Recommender systems aim to fill in this missing information, by predicting the customer score of items where the score is missing. Then recommender systems will recommend items to the customer that have the highest score. \n",
    "\n",
    "This method works by trying to factorise the matrix ùëÖ into two lower dimensional matrices ùëà and ùëâ, so that ùëÖ=ùëà·µÄùëâ.\n",
    "Suppose that R has dimension ùíÖ‚ÇÅ√óùíÖ‚ÇÇ, then U will have dimension ùë´√óùíÖ‚ÇÅ and V will have dimension ùë´√óùíÖ‚ÇÇ. Here ùë´ is chosen by the user, it needs to be large enough to encode the nuances of ùëÖ, but making it too large will make performance slow and could lead to overfitting. A typical size of ùë´ is 20.\n",
    "\n",
    " Imputing the data might work, but it makes the methods very slow. Instead, most popular methods focus only on the matrix entries ùëÖùíæùëó that are known, and fit the factorisation to minimise the error of these known ùëÖùíæùëó. A problem with doing this though is that predictions will be bad because of overfitting. The methods get around this by using a procedure known as regularisation, which is a common way to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2592859f-3800-4c01-b24a-9414facda5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import surprise\n",
    "\n",
    "dataset = pd.read_csv('data/ratings.txt', sep=' ', names = ['uid','iid', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe0b887-b5b5-43c7-be4a-b7e587d64876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>iid</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid  iid  rating\n",
       "0    1    1     2.0\n",
       "1    1    2     4.0\n",
       "2    1    3     3.5\n",
       "3    1    4     3.0\n",
       "4    1    5     4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c92f88-0566-4a2c-918a-54a35cb5e6e1",
   "metadata": {},
   "source": [
    "Not RR matrix - easier to store in sparse format.\n",
    "\n",
    "In a sparse format, the first column is the row number of the matrix ii; the second column is the column number of the matrix jj; and the third row is the matrix entry RijRij. For this dataset, the first column is the user ID, the second is the ID of the movie they‚Äôve reviewed, and the third column is their review score. This sparse format is also the input that matrix factorisation methods require, rather than the full matrix RR, this is because they only use the non-missing matrix entries.\n",
    "\n",
    "### fitting the model\n",
    "First we need to load the dataset into the package surprise, this is done using the Reader class. The main thing the Reader class does is to specify the range of the reviews. Let's first check the range of the reviews for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4f2e3e-ce93-4a1b-9589-51b57ba1146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review range 0.5 to 4.0\n"
     ]
    }
   ],
   "source": [
    "lower_rating = dataset['rating'].min()\n",
    "upper_rating = dataset['rating'].max()\n",
    "print('Review range {0} to {1}'.format(lower_rating, upper_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95829c-744f-4466-84a8-db6d7eadcb47",
   "metadata": {},
   "source": [
    "So our review range goes from 0.5 to 4, which is a little non-standard (the default for surprise is 1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bd155c-8dc9-4900-b0da-5092350497eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = surprise.Reader(rating_scale = (0.5,4.))\n",
    "data = surprise.Dataset.load_from_df(dataset, reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc81e1d-0192-4937-9a25-e377278c7789",
   "metadata": {},
   "source": [
    "We will use the method SVD++, this method extends vanilla SVD algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233cd16f-2873-4e03-9c3f-98d113cb1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = surprise.SVDpp()\n",
    "output = alg.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d05035-cb74-4975-9612-e99df56ef7cb",
   "metadata": {},
   "source": [
    "For now we‚Äôve just trained the model on the whole dataset, which is not good practice but we do it just to give you an idea of how the models and predictions work. Later on we‚Äôll cover proper testing and evaluation; as well as hyperparameter tuning to maximise performance.\n",
    "\n",
    "Now we‚Äôve fitted the model, we can check the predicted score of, for example, user 50 on a music artist 52 using the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d41f5f-342a-46f9-9027-499abd3c96d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0028030537791928\n"
     ]
    }
   ],
   "source": [
    "# The uid and iid should be set as strings\n",
    "pred = alg.predict(uid='50', iid='52')\n",
    "score = pred.est\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eddfcc-510f-43f2-a13f-28e74580e004",
   "metadata": {},
   "source": [
    "So in this case the estimate was a score of 3. But in order to recommend the best products to users, we need to find n items that have the highest predicted score. We'll do this in the next section.\n",
    "\n",
    "## Making Recommendations\n",
    "Let‚Äôs make our recommendations to a particular user. Let‚Äôs focus on uid 50 and find one item to recommend them. First we need to find the movie ids that user 50 didn‚Äôt rate, since we don‚Äôt want to recommend them a movie they‚Äôve already watched!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3eaaa3-f05b-49a6-9f3b-50c8c06467b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all movie ids\n",
    "iids = dataset['iid'].unique()\n",
    "\n",
    "# Get a list of iids that uid 50 has rated\n",
    "iids50 = dataset.loc[dataset['iid'] ==50,'iid']\n",
    "\n",
    "# remove the iids that uid 50 has rated from list of all movie ids\n",
    "iids_to_pred = np.setdiff1d(iids,iids50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7da42f-2022-4df4-bdae-5273cbd21a63",
   "metadata": {},
   "source": [
    "Next we want to predict the score of each of the movie ids that user 50 didn‚Äôt rate, and find the best one. For this we have to create another dataset with the iids we want to predict in the sparse format as before of: uid, iid, rating. We'll just arbitrarily set all the ratings of this test set to 4, as they are not needed. Let's do this, then output the first prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e03c55-9959-4eff-ba7b-e737d57bec38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=50, iid=1, r_ui=4.0, est=3.4381312287143695, details={'was_impossible': False})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = [[50, iid, 4.] for iid in iids_to_pred]\n",
    "predictions = alg.test(testset)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38aab6b-9ffe-4b1c-9b3e-6a09ebb180a3",
   "metadata": {},
   "source": [
    "As you can see from the output, each prediction is a special object. In order to find the best, we‚Äôll convert this object into an array of the predicted ratings. We‚Äôll then use this to find the iid with the best predicted rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c808b8f-7e72-4601-b07b-47505eece5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top item for user 50 has iid 52 with predicted rating 4.0\n"
     ]
    }
   ],
   "source": [
    "pred_ratings = np.array([pred.est for pred in predictions])\n",
    "\n",
    "# find the index of the maximum predicted ratin\n",
    "i_max = pred_ratings.argmax()\n",
    "\n",
    "# Use this to find the corresponding iid to recomment\n",
    "iid = iids_to_pred[i_max]\n",
    "print('Top item for user 50 has iid {0} with predicted rating {1}'.format(iid, pred_ratings[i_max]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce1f0d-d294-47de-ab77-22235a4fb5d1",
   "metadata": {},
   "source": [
    "When you implement your own recommender system you will normally have metadata which allows you to get, for example the name of the film from the iid code. Unfortunately, this dataset does not include this information, but many other larger datasets do, such as the movielens dataset.\n",
    "Similarly you can get the top n items for user 50, just replace the argmax() method with the argpartition() method as per this stackoverflow question.\n",
    "\n",
    "https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "\n",
    "### Tuning and Evaluating the Model\n",
    "As you probably already know, it is bad practice to fit a model on the whole dataset without checking its performance and tuning parameters which affect the fit. So for the remainder of the tutorial we‚Äôll show you how to tune the parameters of SVD++ and evaluate the performance of the method. The method SVD++, as well as most other matrix factorisation algorithms, will depend on a number of main tuning constants: the dimension DD affecting the size of UU and VV; the learning rate, which affects the performance of the optimisation step; the regularisation term affecting the overfitting of the model; and the number of epochs, which determines how many iterations of optimisation are used.\n",
    "\n",
    "#### Grid Search\n",
    "First let‚Äôs define our list of constant values to check, typically the learning rate is a small value between 0 and 1. In theory, the regularisation parameter can be any positive real value, but in practice it is limited as setting it too small will result in overfitting, while setting it too large will result in poor performance; so trying a list of reasonable values should be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f17da7e-50dc-4406-a3fa-086c1675a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr_all': 0.01, 'reg_all': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "param_grid = {'lr_all': [.001, .01], 'reg_all':[.1,.5]}  #n_epochs as well\n",
    "# gs = surprise.model_selection.GridsearchCV(surprise.SVDpp, param_grid, measures-['rsme','mae'], cv=3)\n",
    "gs = GridSearchCV(surprise.SVDpp, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff108e-2ac3-443e-928d-ed034ba0dfc5",
   "metadata": {},
   "source": [
    "The output prints the combination of parameters that gets the best RMSE on a held out test set, RMSE is a way of measuring the prediction error. In this case, we‚Äôve only checked a few tuning constant values, because these procedures can take a while to run. But typically you will try out as many values as possible to get the best performance you can.\n",
    "\n",
    "The performance of a particular model you‚Äôve chosen can be evaluated using cross validation. This might be used to compare a number of methods for example, or just to check your method is performing reasonably. This can be done by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75bbfea6-0f5c-4f55-9528-af13a60bc20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVDpp on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8081  0.7994  0.8106  0.8127  0.7924  0.8046  0.0076  \n",
      "MAE (testset)     0.6173  0.6138  0.6158  0.6145  0.6091  0.6141  0.0028  \n",
      "Fit time          12.09   11.44   11.19   11.25   11.74   11.54   0.33    \n",
      "Test time         0.27    0.25    0.24    0.31    0.28    0.27    0.02    \n"
     ]
    }
   ],
   "source": [
    "alg = surprise.SVDpp(lr_all = 0.01) # param choices added here\n",
    "output = surprise.model_selection.cross_validate(alg, data, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6db27-93ab-477e-b91a-fe59017914b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
